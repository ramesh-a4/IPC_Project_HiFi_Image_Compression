{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9abc049",
   "metadata": {},
   "source": [
    "# HiFiC (Low-memory) — Jupyter Notebook\n",
    "\n",
    "This notebook is a low-memory, GPU-friendly reproduction skeleton for **HiFiC (NeurIPS 2020)**.\n",
    "- Designed to run on a **4 GB GPU** (or CPU) with conservative defaults.\n",
    "- Includes robust LPIPS initialization (falls back if offline), AMP, gradient accumulation, and a flat-image dataloader.\n",
    "\n",
    "Use this notebook to debug and run small experiments. For paper-scale training you will need larger GPUs and longer runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f3b7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: E:\\Software_installation\\Anaconda\\anaconda3\\envs\\ipc\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Install required packages (run once)\n",
    "import sys\n",
    "print('Python executable:', sys.executable)\n",
    "# try:\n",
    "#     # Install only if missing — noisy but useful in fresh envs\n",
    "#     get_ipython().system('pip install --quiet torch torchvision lpips pytorch-fid pytorch-msssim tqdm Pillow scikit-image')\n",
    "# except Exception as e:\n",
    "#     print('Install step skipped or failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a24db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Imports & device\n",
    "import os, math, time, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd4f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Low-memory configuration\n",
    "TRAIN_DIR = 'data/train'   # point to your images (flat folder or ImageFolder layout)\n",
    "TEST_DIR = 'data/test'\n",
    "OUT_DIR = 'outputs'\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "CROP = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_STEPS = 2000\n",
    "SAVE_EVERY = 1000\n",
    "LAMBDA_RATE = 1e-2\n",
    "BETA_GAN = 0.1   # start with 0 (no GAN) to save memory\n",
    "KM = 1.0\n",
    "KP = 0.0         # disable LPIPS during training to save memory (can compute at eval)\n",
    "N_RES = 3\n",
    "BASE_CHANNELS = 24\n",
    "Z_CHANNELS = 64\n",
    "ACCUM_STEPS = 1  # gradient accumulation steps\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71445b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Utilities: FlatImageFolder and demo image creator\n",
    "import glob\n",
    "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "\n",
    "class FlatImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "        for ext in IMG_EXTS:\n",
    "            self.files += glob.glob(os.path.join(root, '**', f'*{ext}'), recursive=True)\n",
    "        self.files = sorted(self.files)\n",
    "        if len(self.files) == 0:\n",
    "            raise FileNotFoundError(f'No images found in {root}')\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0\n",
    "\n",
    "def make_dataloader(path, batch_size=BATCH_SIZE, crop=CROP, shuffle=True):\n",
    "    tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(crop, scale=(0.5,1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    if os.path.exists(path):\n",
    "        # prefer ImageFolder if it has class subfolders\n",
    "        try:\n",
    "            from torchvision import datasets\n",
    "            has_subdirs = any(os.path.isdir(os.path.join(path, d)) for d in os.listdir(path))\n",
    "            if has_subdirs:\n",
    "                ds = datasets.ImageFolder(path, transform=tf)\n",
    "            else:\n",
    "                ds = FlatImageFolder(path, transform=tf)\n",
    "        except Exception:\n",
    "            ds = FlatImageFolder(path, transform=tf)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Path {path} does not exist')\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "def create_demo_images(folder='demo_images', n=8, size=256):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i in range(n):\n",
    "        arr = (np.random.rand(size, size, 3) * 255).astype(np.uint8)\n",
    "        Image.fromarray(arr).save(f\"{folder}/img_{i:03d}.png\")\n",
    "    return folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a985b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Models (low-memory sizes)\n",
    "class ChannelNorm(nn.Module):\n",
    "    def __init__(self, channels, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(channels))\n",
    "        self.beta = nn.Parameter(torch.zeros(channels))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(dim=1, keepdim=True)\n",
    "        var = ((x - mu) ** 2).mean(dim=1, keepdim=True)\n",
    "        sigma = torch.sqrt(var + self.eps)\n",
    "        x_norm = (x - mu) / sigma\n",
    "        a = self.alpha.view(1, -1, 1, 1)\n",
    "        b = self.beta.view(1, -1, 1, 1)\n",
    "        return x_norm * a + b\n",
    "\n",
    "def conv_block(in_ch, out_ch, stride=1, kernel=3, norm=True):\n",
    "    pad = kernel // 2\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, kernel, stride=stride, padding=pad)]\n",
    "    if norm:\n",
    "        layers.append(ChannelNorm(out_ch))\n",
    "    layers.append(nn.ReLU(inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_ch=3, base=BASE_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, base, 7, padding=3), ChannelNorm(base), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base, base*2, 3, stride=2, padding=1), ChannelNorm(base*2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base*2, base*4, 3, stride=2, padding=1), ChannelNorm(base*4), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base*4, base*8, 3, stride=2, padding=1), ChannelNorm(base*8), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.norm1 = ChannelNorm(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.norm2 = ChannelNorm(ch)\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(self.norm1(out))\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        return x + out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, out_ch=3, base=BASE_CHANNELS, n_res=N_RES):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Conv2d(base*8, base*8, 3, padding=1)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(base*8) for _ in range(n_res)])\n",
    "        self.up1 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(base*8, base*4, 3, padding=1), ChannelNorm(base*4), nn.ReLU())\n",
    "        self.up2 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(base*4, base*2, 3, padding=1), ChannelNorm(base*2), nn.ReLU())\n",
    "        self.up3 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(base*2, base, 3, padding=1), ChannelNorm(base), nn.ReLU())\n",
    "        self.final = nn.Conv2d(base, out_ch, 3, padding=1)\n",
    "    def forward(self, y_quant):\n",
    "        x = self.initial(y_quant)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        return torch.clamp(torch.sigmoid(self.final(x)), 0.0, 1.0)\n",
    "\n",
    "class HyperEncoder(nn.Module):\n",
    "    def __init__(self, in_ch=BASE_CHANNELS*8, z_channels=Z_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, in_ch//2, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(in_ch//2, z_channels, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(z_channels, z_channels, 3, stride=2, padding=1),\n",
    "        )\n",
    "    def forward(self, y):\n",
    "        return self.net(y)\n",
    "\n",
    "class HyperDecoder(nn.Module):\n",
    "    def __init__(self, z_channels=Z_CHANNELS, out_ch=BASE_CHANNELS*8*2):\n",
    "        super().__init__()\n",
    "        mid = out_ch//2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_channels, z_channels, 4, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(z_channels, mid, 4, stride=2, padding=1),\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_ch=3, cond_ch=3):\n",
    "        super().__init__()\n",
    "        def conv(cin, cout, k=4, s=2, p=1):\n",
    "            return nn.Sequential(nn.Conv2d(cin, cout, k, stride=s, padding=p), nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.net = nn.Sequential(\n",
    "            conv(in_ch + cond_ch, 64),\n",
    "            conv(64, 128),\n",
    "            conv(128, 256),\n",
    "            conv(256, 512),\n",
    "            nn.Conv2d(512, 1, 1)\n",
    "        )\n",
    "    def forward(self, x, y_cond):\n",
    "        inp = torch.cat([x, y_cond], dim=1)\n",
    "        return self.net(inp)   # RETURN LOGITS (no sigmoid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1c6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Quantization, entropy helpers, and safe ops\n",
    "def add_uniform_noise(x):\n",
    "    return x + (torch.rand_like(x) - 0.5)\n",
    "\n",
    "def round_st(x):\n",
    "    return (x.round() - x).detach() + x\n",
    "\n",
    "def gaussian_log_prob(x, mu, sigma):\n",
    "    eps = 1e-9\n",
    "    var = (sigma + eps) ** 2\n",
    "    log_prob = -0.5 * ( (x - mu)**2 / var + torch.log(2 * math.pi * var) )\n",
    "    return log_prob\n",
    "\n",
    "def estimate_bits(y_quant, mu, sigma):\n",
    "    logp = gaussian_log_prob(y_quant, mu, sigma)\n",
    "    bits = -logp.sum() / math.log(2.0)\n",
    "    return bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d63bb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Software_installation\\Anaconda\\anaconda3\\envs\\ipc\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\Software_installation\\Anaconda\\anaconda3\\envs\\ipc\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: E:\\Software_installation\\Anaconda\\anaconda3\\envs\\ipc\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Software_installation\\Anaconda\\anaconda3\\envs\\ipc\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Losses with robust LPIPS init\n",
    "import warnings\n",
    "try:\n",
    "    import lpips\n",
    "except Exception:\n",
    "    lpips = None\n",
    "\n",
    "def make_lpips(device):\n",
    "    if lpips is None:\n",
    "        warnings.warn('lpips package not available; perceptual loss disabled.')\n",
    "        return None\n",
    "    try:\n",
    "        p = lpips.LPIPS(net='alex')\n",
    "        p = p.to(device)\n",
    "        return p\n",
    "    except Exception as e:\n",
    "        warnings.warn('Could not load pretrained LPIPS network (offline?). Falling back to random init. ' + str(e))\n",
    "        return lpips.LPIPS(net='alex', pnet_rand=True).to(device)\n",
    "\n",
    "LPIPS = make_lpips(DEVICE)\n",
    "\n",
    "bce_logits = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def distortion_loss(x, x_rec, kM=KM, kP=KP):\n",
    "    mse = F.mse_loss(x_rec, x)\n",
    "    lp = torch.tensor(0.0, device=x.device)\n",
    "    if LPIPS is not None and kP > 0:\n",
    "        try:\n",
    "            lp = LPIPS((x*2-1), (x_rec*2-1)).mean()\n",
    "        except Exception as e:\n",
    "            warnings.warn('LPIPS computation failed: ' + str(e))\n",
    "            lp = torch.tensor(0.0, device=x.device)\n",
    "    return kM * mse + kP * lp, float(mse.item()), float(lp.item() if isinstance(lp, torch.Tensor) else lp)\n",
    "\n",
    "def gan_generator_loss(d_fake_logits):\n",
    "    # wants discriminator logits for fake samples\n",
    "    return bce_logits(d_fake_logits, torch.ones_like(d_fake_logits))\n",
    "\n",
    "def gan_discriminator_loss(d_real_logits, d_fake_logits):\n",
    "    # d_real_logits: logits for real images, d_fake_logits: logits for fake images (detached)\n",
    "    loss_real = bce_logits(d_real_logits, torch.ones_like(d_real_logits))\n",
    "    loss_fake = bce_logits(d_fake_logits, torch.zeros_like(d_fake_logits))\n",
    "    return loss_real + loss_fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43e5f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Training loop (AMP + accumulation, low-memory) — corrected AMP usage\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_loop(data_dir=TRAIN_DIR, max_steps=MAX_STEPS):\n",
    "    # prepare dataloader\n",
    "    if not os.path.isdir(data_dir) or len(os.listdir(data_dir))==0:\n",
    "        print('No dataset found at', data_dir, \"— creating demo images in 'demo_images' and using them.\")\n",
    "        demo = create_demo_images('demo_images', n=16, size=256)\n",
    "        data_dir = demo\n",
    "    dl = make_dataloader(data_dir, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # instantiate models\n",
    "    E = Encoder().to(DEVICE)\n",
    "    # generator expects channels BASE_CHANNELS*8\n",
    "    G = Generator(n_res=N_RES).to(DEVICE)\n",
    "    H_enc = HyperEncoder().to(DEVICE)\n",
    "    H_dec = HyperDecoder().to(DEVICE)\n",
    "    D = None\n",
    "    if BETA_GAN > 0:\n",
    "        D = Discriminator(cond_ch=3).to(DEVICE)\n",
    "\n",
    "    optEG = torch.optim.Adam(list(E.parameters()) + list(G.parameters()) + list(H_enc.parameters()) + list(H_dec.parameters()), lr=LEARNING_RATE, betas=(0.9,0.999))\n",
    "    optD = torch.optim.Adam(D.parameters(), lr=LEARNING_RATE, betas=(0.9,0.999)) if D is not None else None\n",
    "\n",
    "    # Safe GradScaler init for different torch versions\n",
    "    try:\n",
    "        scaler = GradScaler(device=torch.device('cuda')) if DEVICE.startswith('cuda') else None\n",
    "    except TypeError:\n",
    "        # older torch versions may not accept device keyword\n",
    "        scaler = GradScaler() if DEVICE.startswith('cuda') else None\n",
    "\n",
    "    # Prepare autocast kwargs\n",
    "    autocast_kwargs = {\"device_type\": \"cuda\"} if DEVICE.startswith('cuda') else {\"device_type\": \"cpu\"}\n",
    "\n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "        for batch in dl:\n",
    "            imgs, _ = batch\n",
    "            imgs = imgs.to(DEVICE)\n",
    "\n",
    "            # Use modern autocast\n",
    "            with autocast(**autocast_kwargs):\n",
    "                y = E(imgs)\n",
    "                y_tilde = add_uniform_noise(y)\n",
    "                z = H_enc(y_tilde)\n",
    "                z_q = round_st(z)\n",
    "                hyper = H_dec(z_q)\n",
    "                C_y = y_tilde.shape[1]\n",
    "                if hyper.shape[1] >= C_y*2:\n",
    "                    mu = hyper[:, :C_y, :, :]\n",
    "                    sigma = F.softplus(hyper[:, C_y:C_y*2, :, :]) + 1e-6\n",
    "                else:\n",
    "                    mu = torch.zeros_like(y_tilde)\n",
    "                    sigma = torch.ones_like(y_tilde)\n",
    "                y_q = round_st(y)\n",
    "                x_rec = G(y_q)\n",
    "                # discriminator conditioning (simple)\n",
    "                y_up = F.interpolate(y_q, size=imgs.shape[2:], mode='nearest')\n",
    "                if y_up.shape[1] >= 3:\n",
    "                    y_cond = y_up[:, :3, :, :].detach()\n",
    "                else:\n",
    "                    y_cond = y_up.repeat(1, 3//y_up.shape[1] + 1, 1, 1)[:, :3, :, :].detach()\n",
    "\n",
    "                # discriminator step (optional)\n",
    "                if D is not None:\n",
    "                    d_real = D(imgs, y_cond)\n",
    "                    d_fake = D(x_rec.detach(), y_cond)\n",
    "                    lossD = gan_discriminator_loss(d_real, d_fake)\n",
    "                else:\n",
    "                    lossD = None\n",
    "\n",
    "                # generator & encoder loss\n",
    "                if D is not None:\n",
    "                    d_fake_forG = D(x_rec, y_cond)\n",
    "                    gen_gan = gan_generator_loss(d_fake_forG)\n",
    "                else:\n",
    "                    gen_gan = 0.0\n",
    "\n",
    "                d_loss, mse_val, lpips_val = distortion_loss(imgs, x_rec, kM=KM, kP=KP)\n",
    "                bits = estimate_bits(y_q, mu, sigma)\n",
    "                rate = bits / (imgs.shape[0] * imgs.shape[2] * imgs.shape[3])\n",
    "                lossG = LAMBDA_RATE * rate + d_loss + BETA_GAN * gen_gan\n",
    "\n",
    "            # backprop (with scaler if using AMP)\n",
    "            if scaler is not None:\n",
    "                scaler.scale(lossG).backward()\n",
    "            else:\n",
    "                lossG.backward()\n",
    "\n",
    "            # step optimizers with accumulation\n",
    "            if (step + 1) % ACCUM_STEPS == 0:\n",
    "                if scaler is not None:\n",
    "                    # step generator/encoder optimizer\n",
    "                    scaler.step(optEG)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optEG.step()\n",
    "                optEG.zero_grad()\n",
    "\n",
    "                # Note: discriminator step was computed earlier; if you enable GAN training (BETA_GAN>0)\n",
    "                # you should add explicit backward/step for optD here (similar to optEG).\n",
    "                if D is not None and lossD is not None:\n",
    "                    # Basic D update (unscaled if no scaler, or scaled if using scaler)\n",
    "                    if scaler is not None:\n",
    "                        scaler.scale(lossD).backward()\n",
    "                        scaler.step(optD)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        lossD.backward()\n",
    "                        optD.step()\n",
    "                    optD.zero_grad()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                # guard float conversion in case lossG is a tensor on CUDA\n",
    "                print(f'Step {step} | lossG {float(lossG):.6f} | rate bpp {float(rate):.6f} | mse {mse_val:.6f} | lpips {lpips_val:.6f}')\n",
    "            if step % SAVE_EVERY == 0 and step>0:\n",
    "                ckpt = {'E':E.state_dict(), 'G':G.state_dict(), 'H_enc':H_enc.state_dict(), 'H_dec':H_dec.state_dict(), 'step': step}\n",
    "                torch.save(ckpt, os.path.join(CHECKPOINT_DIR, f'hific_lowmem_ckpt_{step}.pt'))\n",
    "                print('Saved checkpoint', step)\n",
    "            step += 1\n",
    "            if step >= max_steps:\n",
    "                break\n",
    "    return {'E':E, 'G':G, 'H_enc':H_enc, 'H_dec':H_dec}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7af74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Evaluation helper (PSNR and optional LPIPS)\n",
    "from skimage.metrics import peak_signal_noise_ratio as sk_psnr\n",
    "def save_image_tensor(tensor, filename):\n",
    "    utils.save_image(tensor.clamp(0,1), filename)\n",
    "\n",
    "def psnr_np(a, b, data_range=1.0):\n",
    "    mse = np.mean((a - b) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100.0\n",
    "    return 10 * math.log10((data_range ** 2) / mse)\n",
    "\n",
    "def evaluate_and_save(models, data_dir=TEST_DIR, out_dir=OUT_DIR, n_images=8):\n",
    "    E, G = models['E'], models['G']\n",
    "    E.eval(); G.eval()\n",
    "    if not os.path.isdir(data_dir) or len(os.listdir(data_dir))==0:\n",
    "        data_dir = create_demo_images('demo_images_eval', n=8, size=256)\n",
    "    dl = make_dataloader(data_dir, batch_size=1, crop=CROP, shuffle=False)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    psnrs = []\n",
    "    lpips_vals = []\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for (img,_) in dl:\n",
    "            img = img.to(DEVICE)\n",
    "            y = E(img)\n",
    "            y_q = round_st(y)\n",
    "            rec = G(y_q)\n",
    "            a = img[0].cpu().numpy().transpose(1,2,0)\n",
    "            b = rec[0].cpu().numpy().transpose(1,2,0)\n",
    "            ps = psnr_np(a, b)\n",
    "            psnrs.append(ps)\n",
    "            if LPIPS is not None:\n",
    "                try:\n",
    "                    lp = LPIPS((img*2-1), (rec*2-1)).item()\n",
    "                except Exception:\n",
    "                    lp = 0.0\n",
    "            else:\n",
    "                lp = 0.0\n",
    "            lpips_vals.append(lp)\n",
    "            save_image_tensor(torch.cat([img, rec], dim=0), f\"{out_dir}/recon_{cnt:03d}.png\")\n",
    "            cnt += 1\n",
    "            if cnt >= n_images:\n",
    "                break\n",
    "    print(f\"Avg PSNR: {np.mean(psnrs):.3f} dB | Avg LPIPS: {np.mean(lpips_vals):.6f}\")\n",
    "    return psnrs, lpips_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e7394c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting small demo run...\n",
      "Step 0 | lossG 0.172354 | rate bpp 5.137017 | mse 0.051800 | lpips 0.000000\n",
      "Step 50 | lossG 0.156578 | rate bpp 4.287329 | mse 0.033103 | lpips 0.000000\n",
      "Step 100 | lossG 0.142381 | rate bpp 4.126629 | mse 0.040217 | lpips 0.000000\n",
      "Step 150 | lossG 0.150415 | rate bpp 4.225074 | mse 0.038832 | lpips 0.000000\n",
      "Step 200 | lossG 0.168539 | rate bpp 4.354912 | mse 0.051490 | lpips 0.000000\n",
      "Step 250 | lossG 0.152330 | rate bpp 4.298512 | mse 0.045510 | lpips 0.000000\n",
      "Step 300 | lossG 0.142002 | rate bpp 4.297060 | mse 0.037156 | lpips 0.000000\n",
      "Step 350 | lossG 0.161624 | rate bpp 4.280373 | mse 0.035398 | lpips 0.000000\n",
      "Step 400 | lossG 0.190848 | rate bpp 4.624404 | mse 0.078942 | lpips 0.000000\n",
      "Step 450 | lossG 0.155369 | rate bpp 4.324577 | mse 0.050611 | lpips 0.000000\n",
      "Step 500 | lossG 0.155060 | rate bpp 4.174443 | mse 0.046105 | lpips 0.000000\n",
      "Step 550 | lossG 0.151983 | rate bpp 4.351433 | mse 0.035353 | lpips 0.000000\n",
      "Step 600 | lossG 0.137539 | rate bpp 4.226263 | mse 0.028232 | lpips 0.000000\n",
      "Step 650 | lossG 0.129172 | rate bpp 4.140938 | mse 0.017782 | lpips 0.000000\n",
      "Step 700 | lossG 0.161108 | rate bpp 4.130811 | mse 0.036469 | lpips 0.000000\n",
      "Step 750 | lossG 0.130454 | rate bpp 4.076613 | mse 0.017365 | lpips 0.000000\n",
      "Step 800 | lossG 0.182467 | rate bpp 4.242993 | mse 0.030878 | lpips 0.000000\n",
      "Step 850 | lossG 0.169513 | rate bpp 4.168015 | mse 0.053771 | lpips 0.000000\n",
      "Step 900 | lossG 0.129852 | rate bpp 4.079695 | mse 0.019744 | lpips 0.000000\n",
      "Step 950 | lossG 0.127063 | rate bpp 4.389253 | mse 0.015353 | lpips 0.000000\n",
      "Step 1000 | lossG 0.132433 | rate bpp 4.341924 | mse 0.036711 | lpips 0.000000\n",
      "Saved checkpoint 1000\n",
      "Step 1050 | lossG 0.162346 | rate bpp 4.134994 | mse 0.047608 | lpips 0.000000\n",
      "Step 1100 | lossG 0.151961 | rate bpp 4.335407 | mse 0.047962 | lpips 0.000000\n",
      "Step 1150 | lossG 0.123716 | rate bpp 4.246516 | mse 0.016004 | lpips 0.000000\n",
      "Step 1200 | lossG 0.119810 | rate bpp 4.121742 | mse 0.011772 | lpips 0.000000\n",
      "Step 1250 | lossG 0.123794 | rate bpp 4.081676 | mse 0.016961 | lpips 0.000000\n",
      "Step 1300 | lossG 0.131743 | rate bpp 4.113993 | mse 0.024916 | lpips 0.000000\n",
      "Step 1350 | lossG 0.126936 | rate bpp 4.120377 | mse 0.018761 | lpips 0.000000\n",
      "Step 1400 | lossG 0.123278 | rate bpp 4.071858 | mse 0.015138 | lpips 0.000000\n",
      "Step 1450 | lossG 0.124023 | rate bpp 4.091451 | mse 0.018330 | lpips 0.000000\n",
      "Step 1500 | lossG 0.122983 | rate bpp 4.065034 | mse 0.008618 | lpips 0.000000\n",
      "Step 1550 | lossG 0.140396 | rate bpp 4.160442 | mse 0.026027 | lpips 0.000000\n",
      "Step 1600 | lossG 0.128291 | rate bpp 4.238943 | mse 0.023537 | lpips 0.000000\n",
      "Step 1650 | lossG 0.145997 | rate bpp 4.206318 | mse 0.031168 | lpips 0.000000\n",
      "Step 1700 | lossG 0.136604 | rate bpp 4.291116 | mse 0.026435 | lpips 0.000000\n",
      "Step 1750 | lossG 0.119582 | rate bpp 4.216313 | mse 0.010955 | lpips 0.000000\n",
      "Step 1800 | lossG 0.126252 | rate bpp 4.114653 | mse 0.027150 | lpips 0.000000\n",
      "Step 1850 | lossG 0.126214 | rate bpp 4.093872 | mse 0.018790 | lpips 0.000000\n",
      "Step 1900 | lossG 0.132456 | rate bpp 4.145429 | mse 0.026087 | lpips 0.000000\n",
      "Step 1950 | lossG 0.122539 | rate bpp 4.259240 | mse 0.014999 | lpips 0.000000\n",
      "Step 2000 | lossG 0.119062 | rate bpp 4.200375 | mse 0.011463 | lpips 0.000000\n",
      "Saved checkpoint 2000\n",
      "Step 2050 | lossG 0.144612 | rate bpp 4.175896 | mse 0.023430 | lpips 0.000000\n",
      "Step 2100 | lossG 0.110987 | rate bpp 4.174619 | mse 0.019128 | lpips 0.000000\n",
      "Step 2150 | lossG 0.122254 | rate bpp 4.136138 | mse 0.014429 | lpips 0.000000\n",
      "Step 2200 | lossG 0.146068 | rate bpp 4.094973 | mse 0.030576 | lpips 0.000000\n",
      "Step 2250 | lossG 0.168548 | rate bpp 4.156611 | mse 0.029834 | lpips 0.000000\n",
      "Step 2300 | lossG 0.113361 | rate bpp 4.172329 | mse 0.008659 | lpips 0.000000\n",
      "Step 2350 | lossG 0.111225 | rate bpp 4.138296 | mse 0.015986 | lpips 0.000000\n",
      "Step 2400 | lossG 0.112152 | rate bpp 4.156391 | mse 0.008203 | lpips 0.000000\n",
      "Step 2450 | lossG 0.121713 | rate bpp 4.154762 | mse 0.013691 | lpips 0.000000\n",
      "Step 2500 | lossG 0.118304 | rate bpp 4.153133 | mse 0.010611 | lpips 0.000000\n",
      "Step 2550 | lossG 0.136585 | rate bpp 4.173694 | mse 0.022149 | lpips 0.000000\n",
      "Step 2600 | lossG 0.136024 | rate bpp 4.156744 | mse 0.011818 | lpips 0.000000\n",
      "Step 2650 | lossG 0.118590 | rate bpp 4.093828 | mse 0.012632 | lpips 0.000000\n",
      "Step 2700 | lossG 0.113805 | rate bpp 4.135346 | mse 0.012433 | lpips 0.000000\n",
      "Step 2750 | lossG 0.114853 | rate bpp 4.178493 | mse 0.008762 | lpips 0.000000\n",
      "Step 2800 | lossG 0.104858 | rate bpp 4.187167 | mse 0.009877 | lpips 0.000000\n",
      "Step 2850 | lossG 0.122563 | rate bpp 4.156127 | mse 0.016962 | lpips 0.000000\n",
      "Step 2900 | lossG 0.112217 | rate bpp 4.082425 | mse 0.005419 | lpips 0.000000\n",
      "Step 2950 | lossG 0.114099 | rate bpp 4.096162 | mse 0.005856 | lpips 0.000000\n",
      "Step 3000 | lossG 0.121702 | rate bpp 4.123063 | mse 0.022870 | lpips 0.000000\n",
      "Saved checkpoint 3000\n",
      "Step 3050 | lossG 0.110655 | rate bpp 4.115226 | mse 0.007925 | lpips 0.000000\n",
      "Step 3100 | lossG 0.120252 | rate bpp 4.125968 | mse 0.021433 | lpips 0.000000\n",
      "Step 3150 | lossG 0.122706 | rate bpp 4.094489 | mse 0.022007 | lpips 0.000000\n",
      "Step 3200 | lossG 0.138424 | rate bpp 4.239251 | mse 0.025525 | lpips 0.000000\n",
      "Step 3250 | lossG 0.121259 | rate bpp 4.091098 | mse 0.011971 | lpips 0.000000\n",
      "Step 3300 | lossG 0.119063 | rate bpp 4.150316 | mse 0.011528 | lpips 0.000000\n",
      "Step 3350 | lossG 0.134985 | rate bpp 4.250610 | mse 0.015732 | lpips 0.000000\n",
      "Step 3400 | lossG 0.132302 | rate bpp 4.135390 | mse 0.019977 | lpips 0.000000\n",
      "Step 3450 | lossG 0.103121 | rate bpp 4.099288 | mse 0.006844 | lpips 0.000000\n",
      "Step 3500 | lossG 0.118223 | rate bpp 4.133673 | mse 0.015842 | lpips 0.000000\n",
      "Step 3550 | lossG 0.130759 | rate bpp 4.085727 | mse 0.015030 | lpips 0.000000\n",
      "Step 3600 | lossG 0.120510 | rate bpp 4.203457 | mse 0.013785 | lpips 0.000000\n",
      "Step 3650 | lossG 0.120710 | rate bpp 4.149347 | mse 0.010387 | lpips 0.000000\n",
      "Step 3700 | lossG 0.115349 | rate bpp 4.120421 | mse 0.008972 | lpips 0.000000\n",
      "Step 3750 | lossG 0.134259 | rate bpp 4.227936 | mse 0.020968 | lpips 0.000000\n",
      "Step 3800 | lossG 0.136273 | rate bpp 4.168147 | mse 0.021145 | lpips 0.000000\n",
      "Step 3850 | lossG 0.117094 | rate bpp 4.131340 | mse 0.008420 | lpips 0.000000\n",
      "Step 3900 | lossG 0.104186 | rate bpp 4.077978 | mse 0.008211 | lpips 0.000000\n",
      "Step 3950 | lossG 0.115892 | rate bpp 4.086475 | mse 0.008548 | lpips 0.000000\n",
      "Step 4000 | lossG 0.147955 | rate bpp 4.122931 | mse 0.015066 | lpips 0.000000\n",
      "Saved checkpoint 4000\n",
      "Step 4050 | lossG 0.128638 | rate bpp 4.170656 | mse 0.019181 | lpips 0.000000\n",
      "Step 4100 | lossG 0.125015 | rate bpp 4.152297 | mse 0.014986 | lpips 0.000000\n",
      "Step 4150 | lossG 0.108476 | rate bpp 4.183776 | mse 0.023299 | lpips 0.000000\n",
      "Step 4200 | lossG 0.112803 | rate bpp 4.095237 | mse 0.006606 | lpips 0.000000\n",
      "Step 4250 | lossG 0.111174 | rate bpp 4.171096 | mse 0.011493 | lpips 0.000000\n",
      "Step 4300 | lossG 0.119821 | rate bpp 4.143447 | mse 0.015317 | lpips 0.000000\n",
      "Step 4350 | lossG 0.116876 | rate bpp 4.169027 | mse 0.009546 | lpips 0.000000\n",
      "Step 4400 | lossG 0.112849 | rate bpp 4.139485 | mse 0.006456 | lpips 0.000000\n",
      "Step 4450 | lossG 0.111075 | rate bpp 4.159077 | mse 0.007939 | lpips 0.000000\n",
      "Step 4500 | lossG 0.129355 | rate bpp 4.106112 | mse 0.018538 | lpips 0.000000\n",
      "Step 4550 | lossG 0.124143 | rate bpp 4.181003 | mse 0.011435 | lpips 0.000000\n",
      "Step 4600 | lossG 0.120604 | rate bpp 4.097306 | mse 0.008197 | lpips 0.000000\n",
      "Step 4650 | lossG 0.144062 | rate bpp 4.197029 | mse 0.012825 | lpips 0.000000\n",
      "Step 4700 | lossG 0.126391 | rate bpp 4.173034 | mse 0.013914 | lpips 0.000000\n",
      "Step 4750 | lossG 0.108617 | rate bpp 4.156215 | mse 0.013252 | lpips 0.000000\n",
      "Step 4800 | lossG 0.122668 | rate bpp 4.227892 | mse 0.009570 | lpips 0.000000\n",
      "Step 4850 | lossG 0.113238 | rate bpp 4.115049 | mse 0.005377 | lpips 0.000000\n",
      "Step 4900 | lossG 0.129669 | rate bpp 4.075733 | mse 0.007981 | lpips 0.000000\n",
      "Step 4950 | lossG 0.124359 | rate bpp 4.114697 | mse 0.010637 | lpips 0.000000\n",
      "Step 5000 | lossG 0.130414 | rate bpp 4.193771 | mse 0.014667 | lpips 0.000000\n",
      "Saved checkpoint 5000\n",
      "Step 5050 | lossG 0.107495 | rate bpp 4.176116 | mse 0.017387 | lpips 0.000000\n",
      "Step 5100 | lossG 0.120843 | rate bpp 4.094313 | mse 0.012270 | lpips 0.000000\n",
      "Step 5150 | lossG 0.113642 | rate bpp 4.113420 | mse 0.005940 | lpips 0.000000\n",
      "Step 5200 | lossG 0.113233 | rate bpp 4.122534 | mse 0.006367 | lpips 0.000000\n",
      "Step 5250 | lossG 0.112502 | rate bpp 4.117603 | mse 0.007230 | lpips 0.000000\n",
      "Step 5300 | lossG 0.113798 | rate bpp 4.146397 | mse 0.011011 | lpips 0.000000\n",
      "Step 5350 | lossG 0.120061 | rate bpp 4.250478 | mse 0.019403 | lpips 0.000000\n",
      "Step 5400 | lossG 0.121089 | rate bpp 4.185846 | mse 0.010954 | lpips 0.000000\n",
      "Step 5450 | lossG 0.139920 | rate bpp 4.152913 | mse 0.022750 | lpips 0.000000\n",
      "Step 5500 | lossG 0.157237 | rate bpp 4.194828 | mse 0.027872 | lpips 0.000000\n",
      "Step 5550 | lossG 0.117301 | rate bpp 4.129711 | mse 0.009310 | lpips 0.000000\n",
      "Step 5600 | lossG 0.133454 | rate bpp 4.197073 | mse 0.019426 | lpips 0.000000\n",
      "Step 5650 | lossG 0.114813 | rate bpp 4.116062 | mse 0.006579 | lpips 0.000000\n",
      "Step 5700 | lossG 0.114812 | rate bpp 4.158285 | mse 0.008319 | lpips 0.000000\n",
      "Step 5750 | lossG 0.115536 | rate bpp 4.152341 | mse 0.009935 | lpips 0.000000\n",
      "Step 5800 | lossG 0.113937 | rate bpp 4.138780 | mse 0.007118 | lpips 0.000000\n",
      "Step 5850 | lossG 0.112763 | rate bpp 4.154762 | mse 0.009095 | lpips 0.000000\n",
      "Step 5900 | lossG 0.114338 | rate bpp 4.116150 | mse 0.005662 | lpips 0.000000\n",
      "Step 5950 | lossG 0.118532 | rate bpp 4.141246 | mse 0.012382 | lpips 0.000000\n",
      "Step 6000 | lossG 0.116149 | rate bpp 4.115842 | mse 0.008330 | lpips 0.000000\n",
      "Saved checkpoint 6000\n",
      "Step 6050 | lossG 0.127409 | rate bpp 4.151901 | mse 0.017282 | lpips 0.000000\n",
      "Step 6100 | lossG 0.107971 | rate bpp 4.101005 | mse 0.005454 | lpips 0.000000\n",
      "Step 6150 | lossG 0.109449 | rate bpp 4.151152 | mse 0.008963 | lpips 0.000000\n",
      "Step 6200 | lossG 0.120046 | rate bpp 4.152649 | mse 0.013675 | lpips 0.000000\n",
      "Step 6250 | lossG 0.110581 | rate bpp 4.120817 | mse 0.006188 | lpips 0.000000\n",
      "Step 6300 | lossG 0.120886 | rate bpp 4.094049 | mse 0.018426 | lpips 0.000000\n",
      "Step 6350 | lossG 0.120331 | rate bpp 4.116679 | mse 0.006228 | lpips 0.000000\n",
      "Step 6400 | lossG 0.117566 | rate bpp 4.098715 | mse 0.006059 | lpips 0.000000\n",
      "Step 6450 | lossG 0.126391 | rate bpp 4.141246 | mse 0.014681 | lpips 0.000000\n",
      "Step 6500 | lossG 0.110775 | rate bpp 4.128346 | mse 0.009463 | lpips 0.000000\n",
      "Step 6550 | lossG 0.112939 | rate bpp 4.106464 | mse 0.007494 | lpips 0.000000\n",
      "Step 6600 | lossG 0.128200 | rate bpp 4.141686 | mse 0.015785 | lpips 0.000000\n",
      "Step 6650 | lossG 0.128951 | rate bpp 4.064065 | mse 0.010612 | lpips 0.000000\n",
      "Step 6700 | lossG 0.121563 | rate bpp 4.153794 | mse 0.008967 | lpips 0.000000\n",
      "Step 6750 | lossG 0.116119 | rate bpp 4.116415 | mse 0.011599 | lpips 0.000000\n",
      "Step 6800 | lossG 0.142140 | rate bpp 4.212439 | mse 0.019146 | lpips 0.000000\n",
      "Step 6850 | lossG 0.123011 | rate bpp 4.174707 | mse 0.013501 | lpips 0.000000\n",
      "Step 6900 | lossG 0.123340 | rate bpp 4.137768 | mse 0.014578 | lpips 0.000000\n",
      "Step 6950 | lossG 0.120523 | rate bpp 4.141554 | mse 0.012376 | lpips 0.000000\n",
      "Step 7000 | lossG 0.125531 | rate bpp 4.069481 | mse 0.010593 | lpips 0.000000\n",
      "Saved checkpoint 7000\n",
      "Step 7050 | lossG 0.127868 | rate bpp 4.207111 | mse 0.019064 | lpips 0.000000\n",
      "Step 7100 | lossG 0.130435 | rate bpp 4.112540 | mse 0.005953 | lpips 0.000000\n",
      "Step 7150 | lossG 0.112139 | rate bpp 4.155070 | mse 0.009325 | lpips 0.000000\n",
      "Step 7200 | lossG 0.112315 | rate bpp 4.095721 | mse 0.006538 | lpips 0.000000\n",
      "Step 7250 | lossG 0.142432 | rate bpp 4.152209 | mse 0.024745 | lpips 0.000000\n",
      "Step 7300 | lossG 0.116647 | rate bpp 4.165461 | mse 0.010722 | lpips 0.000000\n",
      "Step 7350 | lossG 0.117494 | rate bpp 4.135698 | mse 0.016800 | lpips 0.000000\n",
      "Step 7400 | lossG 0.117669 | rate bpp 4.133365 | mse 0.008597 | lpips 0.000000\n",
      "Step 7450 | lossG 0.112267 | rate bpp 4.095721 | mse 0.006318 | lpips 0.000000\n",
      "Step 7500 | lossG 0.115549 | rate bpp 4.153441 | mse 0.013718 | lpips 0.000000\n",
      "Step 7550 | lossG 0.133991 | rate bpp 4.154894 | mse 0.015375 | lpips 0.000000\n",
      "Step 7600 | lossG 0.103360 | rate bpp 4.133981 | mse 0.009379 | lpips 0.000000\n",
      "Step 7650 | lossG 0.109838 | rate bpp 4.083526 | mse 0.006439 | lpips 0.000000\n",
      "Step 7700 | lossG 0.115617 | rate bpp 4.160222 | mse 0.009739 | lpips 0.000000\n",
      "Step 7750 | lossG 0.121848 | rate bpp 4.078947 | mse 0.008710 | lpips 0.000000\n",
      "Step 7800 | lossG 0.174098 | rate bpp 4.170304 | mse 0.033950 | lpips 0.000000\n",
      "Step 7850 | lossG 0.118074 | rate bpp 4.162467 | mse 0.010534 | lpips 0.000000\n",
      "Step 7900 | lossG 0.124621 | rate bpp 4.132088 | mse 0.010351 | lpips 0.000000\n",
      "Step 7950 | lossG 0.123691 | rate bpp 4.107697 | mse 0.011241 | lpips 0.000000\n",
      "Step 8000 | lossG 0.114955 | rate bpp 4.080004 | mse 0.006197 | lpips 0.000000\n",
      "Saved checkpoint 8000\n",
      "Step 8050 | lossG 0.125963 | rate bpp 4.127069 | mse 0.014871 | lpips 0.000000\n",
      "Step 8100 | lossG 0.114761 | rate bpp 4.195884 | mse 0.010594 | lpips 0.000000\n",
      "Step 8150 | lossG 0.130389 | rate bpp 4.091847 | mse 0.012289 | lpips 0.000000\n",
      "Step 8200 | lossG 0.110969 | rate bpp 4.083042 | mse 0.006634 | lpips 0.000000\n",
      "Step 8250 | lossG 0.110578 | rate bpp 4.074985 | mse 0.007627 | lpips 0.000000\n",
      "Step 8300 | lossG 0.120897 | rate bpp 4.134158 | mse 0.015602 | lpips 0.000000\n",
      "Step 8350 | lossG 0.114494 | rate bpp 4.111924 | mse 0.010349 | lpips 0.000000\n",
      "Step 8400 | lossG 0.131898 | rate bpp 4.064286 | mse 0.012346 | lpips 0.000000\n",
      "Step 8450 | lossG 0.111780 | rate bpp 4.113464 | mse 0.005442 | lpips 0.000000\n",
      "Step 8500 | lossG 0.136335 | rate bpp 4.144063 | mse 0.016603 | lpips 0.000000\n",
      "Step 8550 | lossG 0.116348 | rate bpp 4.110162 | mse 0.006653 | lpips 0.000000\n",
      "Step 8600 | lossG 0.115198 | rate bpp 4.105231 | mse 0.007165 | lpips 0.000000\n",
      "Step 8650 | lossG 0.116839 | rate bpp 4.086520 | mse 0.004410 | lpips 0.000000\n",
      "Step 8700 | lossG 0.120526 | rate bpp 4.131868 | mse 0.008686 | lpips 0.000000\n",
      "Step 8750 | lossG 0.118329 | rate bpp 4.116238 | mse 0.011199 | lpips 0.000000\n",
      "Step 8800 | lossG 0.116802 | rate bpp 4.149875 | mse 0.006811 | lpips 0.000000\n",
      "Step 8850 | lossG 0.117517 | rate bpp 4.133805 | mse 0.008460 | lpips 0.000000\n",
      "Step 8900 | lossG 0.113612 | rate bpp 4.149215 | mse 0.010824 | lpips 0.000000\n",
      "Step 8950 | lossG 0.121032 | rate bpp 4.182588 | mse 0.013955 | lpips 0.000000\n",
      "Step 9000 | lossG 0.118904 | rate bpp 4.134598 | mse 0.011000 | lpips 0.000000\n",
      "Saved checkpoint 9000\n",
      "Step 9050 | lossG 0.124786 | rate bpp 4.133541 | mse 0.009462 | lpips 0.000000\n",
      "Step 9100 | lossG 0.114843 | rate bpp 4.124691 | mse 0.005679 | lpips 0.000000\n",
      "Step 9150 | lossG 0.124496 | rate bpp 4.104439 | mse 0.014632 | lpips 0.000000\n",
      "Step 9200 | lossG 0.111200 | rate bpp 4.106596 | mse 0.003331 | lpips 0.000000\n",
      "Step 9250 | lossG 0.114938 | rate bpp 4.090174 | mse 0.007466 | lpips 0.000000\n",
      "Step 9300 | lossG 0.116891 | rate bpp 4.179198 | mse 0.010352 | lpips 0.000000\n",
      "Step 9350 | lossG 0.114784 | rate bpp 4.109326 | mse 0.007328 | lpips 0.000000\n",
      "Step 9400 | lossG 0.120619 | rate bpp 4.126101 | mse 0.023110 | lpips 0.000000\n",
      "Step 9450 | lossG 0.117186 | rate bpp 4.123899 | mse 0.009666 | lpips 0.000000\n",
      "Step 9500 | lossG 0.113146 | rate bpp 4.120553 | mse 0.005697 | lpips 0.000000\n",
      "Step 9550 | lossG 0.130214 | rate bpp 4.100168 | mse 0.014219 | lpips 0.000000\n",
      "Step 9600 | lossG 0.107466 | rate bpp 4.181267 | mse 0.005782 | lpips 0.000000\n",
      "Step 9650 | lossG 0.114012 | rate bpp 4.118219 | mse 0.006895 | lpips 0.000000\n",
      "Step 9700 | lossG 0.115242 | rate bpp 4.166870 | mse 0.010697 | lpips 0.000000\n",
      "Step 9750 | lossG 0.122797 | rate bpp 4.120509 | mse 0.010205 | lpips 0.000000\n",
      "Step 9800 | lossG 0.122997 | rate bpp 4.113553 | mse 0.015097 | lpips 0.000000\n",
      "Step 9850 | lossG 0.115017 | rate bpp 4.105011 | mse 0.003537 | lpips 0.000000\n",
      "Step 9900 | lossG 0.112637 | rate bpp 4.161014 | mse 0.016239 | lpips 0.000000\n",
      "Step 9950 | lossG 0.117569 | rate bpp 4.174047 | mse 0.011373 | lpips 0.000000\n",
      "Step 10000 | lossG 0.117662 | rate bpp 4.131736 | mse 0.011219 | lpips 0.000000\n",
      "Saved checkpoint 10000\n",
      "Step 10050 | lossG 0.114483 | rate bpp 4.114521 | mse 0.010337 | lpips 0.000000\n",
      "Step 10100 | lossG 0.118339 | rate bpp 4.089602 | mse 0.012330 | lpips 0.000000\n",
      "Step 10150 | lossG 0.116530 | rate bpp 4.161191 | mse 0.011705 | lpips 0.000000\n",
      "Step 10200 | lossG 0.119050 | rate bpp 4.106905 | mse 0.010520 | lpips 0.000000\n",
      "Step 10250 | lossG 0.113375 | rate bpp 4.111571 | mse 0.008131 | lpips 0.000000\n",
      "Step 10300 | lossG 0.113883 | rate bpp 4.107080 | mse 0.006736 | lpips 0.000000\n",
      "Step 10350 | lossG 0.118178 | rate bpp 4.091803 | mse 0.008072 | lpips 0.000000\n",
      "Step 10400 | lossG 0.121391 | rate bpp 4.098759 | mse 0.007262 | lpips 0.000000\n",
      "Step 10450 | lossG 0.134663 | rate bpp 4.178185 | mse 0.020718 | lpips 0.000000\n",
      "Step 10500 | lossG 0.112686 | rate bpp 4.118395 | mse 0.006431 | lpips 0.000000\n",
      "Step 10550 | lossG 0.119205 | rate bpp 4.132705 | mse 0.013313 | lpips 0.000000\n",
      "Step 10600 | lossG 0.122124 | rate bpp 4.065431 | mse 0.010507 | lpips 0.000000\n",
      "Step 10650 | lossG 0.118872 | rate bpp 4.113641 | mse 0.011537 | lpips 0.000000\n",
      "Step 10700 | lossG 0.117504 | rate bpp 4.144592 | mse 0.011924 | lpips 0.000000\n",
      "Step 10750 | lossG 0.117418 | rate bpp 4.076834 | mse 0.008290 | lpips 0.000000\n",
      "Step 10800 | lossG 0.110145 | rate bpp 4.117911 | mse 0.006642 | lpips 0.000000\n",
      "Step 10850 | lossG 0.115333 | rate bpp 4.155643 | mse 0.009706 | lpips 0.000000\n",
      "Step 10900 | lossG 0.112526 | rate bpp 4.159825 | mse 0.008643 | lpips 0.000000\n",
      "Step 10950 | lossG 0.111379 | rate bpp 4.132176 | mse 0.005164 | lpips 0.000000\n",
      "Step 11000 | lossG 0.114447 | rate bpp 4.122754 | mse 0.010272 | lpips 0.000000\n",
      "Saved checkpoint 11000\n",
      "Step 11050 | lossG 0.111724 | rate bpp 4.106464 | mse 0.005019 | lpips 0.000000\n",
      "Step 11100 | lossG 0.121948 | rate bpp 4.148246 | mse 0.013027 | lpips 0.000000\n",
      "Step 11150 | lossG 0.110977 | rate bpp 4.126673 | mse 0.004205 | lpips 0.000000\n",
      "Step 11200 | lossG 0.125755 | rate bpp 4.103514 | mse 0.007972 | lpips 0.000000\n",
      "Step 11250 | lossG 0.109909 | rate bpp 4.141334 | mse 0.004543 | lpips 0.000000\n",
      "Step 11300 | lossG 0.116805 | rate bpp 4.111703 | mse 0.008177 | lpips 0.000000\n",
      "Step 11350 | lossG 0.109910 | rate bpp 4.105980 | mse 0.003958 | lpips 0.000000\n",
      "Step 11400 | lossG 0.113377 | rate bpp 4.110118 | mse 0.007048 | lpips 0.000000\n",
      "Step 11450 | lossG 0.115073 | rate bpp 4.113024 | mse 0.008060 | lpips 0.000000\n",
      "Step 11500 | lossG 0.137983 | rate bpp 4.111263 | mse 0.013999 | lpips 0.000000\n",
      "Step 11550 | lossG 0.116280 | rate bpp 4.153882 | mse 0.010133 | lpips 0.000000\n",
      "Step 11600 | lossG 0.114503 | rate bpp 4.130856 | mse 0.008843 | lpips 0.000000\n",
      "Step 11650 | lossG 0.118095 | rate bpp 4.100653 | mse 0.009368 | lpips 0.000000\n",
      "Step 11700 | lossG 0.119451 | rate bpp 4.140585 | mse 0.008956 | lpips 0.000000\n",
      "Step 11750 | lossG 0.113237 | rate bpp 4.116987 | mse 0.006752 | lpips 0.000000\n",
      "Step 11800 | lossG 0.114516 | rate bpp 4.141862 | mse 0.006963 | lpips 0.000000\n",
      "Step 11850 | lossG 0.118347 | rate bpp 4.175983 | mse 0.011237 | lpips 0.000000\n",
      "Step 11900 | lossG 0.114611 | rate bpp 4.140277 | mse 0.008991 | lpips 0.000000\n",
      "Step 11950 | lossG 0.115017 | rate bpp 4.111043 | mse 0.010233 | lpips 0.000000\n",
      "Step 12000 | lossG 0.144398 | rate bpp 4.163304 | mse 0.018519 | lpips 0.000000\n",
      "Saved checkpoint 12000\n",
      "Step 12050 | lossG 0.126105 | rate bpp 4.157448 | mse 0.009469 | lpips 0.000000\n",
      "Step 12100 | lossG 0.116504 | rate bpp 4.164316 | mse 0.011027 | lpips 0.000000\n",
      "Step 12150 | lossG 0.118434 | rate bpp 4.159561 | mse 0.009404 | lpips 0.000000\n",
      "Step 12200 | lossG 0.109041 | rate bpp 4.142435 | mse 0.009016 | lpips 0.000000\n",
      "Step 12250 | lossG 0.124458 | rate bpp 4.153617 | mse 0.012170 | lpips 0.000000\n",
      "Step 12300 | lossG 0.110916 | rate bpp 4.105319 | mse 0.008359 | lpips 0.000000\n",
      "Step 12350 | lossG 0.106209 | rate bpp 4.096294 | mse 0.007700 | lpips 0.000000\n",
      "Step 12400 | lossG 0.109220 | rate bpp 4.076966 | mse 0.008288 | lpips 0.000000\n",
      "Step 12450 | lossG 0.113295 | rate bpp 4.190073 | mse 0.012146 | lpips 0.000000\n",
      "Step 12500 | lossG 0.120330 | rate bpp 4.165329 | mse 0.011536 | lpips 0.000000\n",
      "Step 12550 | lossG 0.103318 | rate bpp 4.113068 | mse 0.013663 | lpips 0.000000\n",
      "Step 12600 | lossG 0.116669 | rate bpp 4.126101 | mse 0.005717 | lpips 0.000000\n",
      "Step 12650 | lossG 0.116117 | rate bpp 4.105628 | mse 0.020653 | lpips 0.000000\n",
      "Step 12700 | lossG 0.109483 | rate bpp 4.122534 | mse 0.003303 | lpips 0.000000\n",
      "Step 12750 | lossG 0.115308 | rate bpp 4.122490 | mse 0.007308 | lpips 0.000000\n",
      "Step 12800 | lossG 0.113116 | rate bpp 4.115358 | mse 0.010319 | lpips 0.000000\n",
      "Step 12850 | lossG 0.103275 | rate bpp 4.126452 | mse 0.006148 | lpips 0.000000\n",
      "Step 12900 | lossG 0.117742 | rate bpp 4.151901 | mse 0.007475 | lpips 0.000000\n",
      "Step 12950 | lossG 0.116582 | rate bpp 4.140277 | mse 0.009383 | lpips 0.000000\n",
      "Step 13000 | lossG 0.117540 | rate bpp 4.130239 | mse 0.006519 | lpips 0.000000\n",
      "Saved checkpoint 13000\n",
      "Step 13050 | lossG 0.118915 | rate bpp 4.173782 | mse 0.012817 | lpips 0.000000\n",
      "Step 13100 | lossG 0.113261 | rate bpp 4.132881 | mse 0.008115 | lpips 0.000000\n",
      "Step 13150 | lossG 0.113166 | rate bpp 4.104835 | mse 0.011343 | lpips 0.000000\n",
      "Step 13200 | lossG 0.123006 | rate bpp 4.138076 | mse 0.014620 | lpips 0.000000\n",
      "Step 13250 | lossG 0.115166 | rate bpp 4.107124 | mse 0.005568 | lpips 0.000000\n",
      "Step 13300 | lossG 0.122121 | rate bpp 4.143976 | mse 0.014194 | lpips 0.000000\n",
      "Step 13350 | lossG 0.111290 | rate bpp 4.116150 | mse 0.004897 | lpips 0.000000\n",
      "Step 13400 | lossG 0.116461 | rate bpp 4.122754 | mse 0.006413 | lpips 0.000000\n",
      "Step 13450 | lossG 0.114302 | rate bpp 4.120421 | mse 0.006990 | lpips 0.000000\n",
      "Step 13500 | lossG 0.116639 | rate bpp 4.091715 | mse 0.007351 | lpips 0.000000\n",
      "Step 13550 | lossG 0.113210 | rate bpp 4.127025 | mse 0.006251 | lpips 0.000000\n",
      "Step 13600 | lossG 0.120657 | rate bpp 4.156744 | mse 0.014977 | lpips 0.000000\n",
      "Step 13650 | lossG 0.123593 | rate bpp 4.115798 | mse 0.015561 | lpips 0.000000\n",
      "Step 13700 | lossG 0.111685 | rate bpp 4.125220 | mse 0.007116 | lpips 0.000000\n",
      "Step 13750 | lossG 0.113370 | rate bpp 4.182191 | mse 0.011102 | lpips 0.000000\n",
      "Step 13800 | lossG 0.118483 | rate bpp 4.097879 | mse 0.007631 | lpips 0.000000\n",
      "Step 13850 | lossG 0.112770 | rate bpp 4.101885 | mse 0.004254 | lpips 0.000000\n",
      "Step 13900 | lossG 0.111163 | rate bpp 4.087004 | mse 0.005576 | lpips 0.000000\n",
      "Step 13950 | lossG 0.115690 | rate bpp 4.103646 | mse 0.006447 | lpips 0.000000\n",
      "Step 14000 | lossG 0.118853 | rate bpp 4.116634 | mse 0.006242 | lpips 0.000000\n",
      "Saved checkpoint 14000\n",
      "Step 14050 | lossG 0.113024 | rate bpp 4.148995 | mse 0.008556 | lpips 0.000000\n",
      "Step 14100 | lossG 0.111267 | rate bpp 4.131472 | mse 0.005436 | lpips 0.000000\n",
      "Step 14150 | lossG 0.110935 | rate bpp 4.116326 | mse 0.004353 | lpips 0.000000\n",
      "Step 14200 | lossG 0.114772 | rate bpp 4.134202 | mse 0.006964 | lpips 0.000000\n",
      "Step 14250 | lossG 0.113305 | rate bpp 4.118263 | mse 0.004693 | lpips 0.000000\n",
      "Step 14300 | lossG 0.114460 | rate bpp 4.121478 | mse 0.007945 | lpips 0.000000\n",
      "Step 14350 | lossG 0.118318 | rate bpp 4.125968 | mse 0.013590 | lpips 0.000000\n",
      "Step 14400 | lossG 0.109227 | rate bpp 4.147101 | mse 0.006407 | lpips 0.000000\n",
      "Step 14450 | lossG 0.134559 | rate bpp 4.107477 | mse 0.015986 | lpips 0.000000\n",
      "Step 14500 | lossG 0.113852 | rate bpp 4.134290 | mse 0.006267 | lpips 0.000000\n",
      "Step 14550 | lossG 0.111160 | rate bpp 4.096998 | mse 0.003250 | lpips 0.000000\n",
      "Step 14600 | lossG 0.117417 | rate bpp 4.113509 | mse 0.010186 | lpips 0.000000\n",
      "Step 14650 | lossG 0.118475 | rate bpp 4.140982 | mse 0.007195 | lpips 0.000000\n",
      "Step 14700 | lossG 0.113603 | rate bpp 4.162071 | mse 0.007543 | lpips 0.000000\n",
      "Step 14750 | lossG 0.114431 | rate bpp 4.085947 | mse 0.005815 | lpips 0.000000\n",
      "Step 14800 | lossG 0.110242 | rate bpp 4.097086 | mse 0.004693 | lpips 0.000000\n",
      "Step 14850 | lossG 0.122476 | rate bpp 4.112100 | mse 0.015051 | lpips 0.000000\n",
      "Step 14900 | lossG 0.113561 | rate bpp 4.126233 | mse 0.008934 | lpips 0.000000\n",
      "Step 14950 | lossG 0.140036 | rate bpp 4.078947 | mse 0.017605 | lpips 0.000000\n",
      "Step 15000 | lossG 0.110349 | rate bpp 4.135874 | mse 0.004174 | lpips 0.000000\n",
      "Saved checkpoint 15000\n",
      "Step 15050 | lossG 0.112399 | rate bpp 4.162995 | mse 0.010059 | lpips 0.000000\n",
      "Step 15100 | lossG 0.126129 | rate bpp 4.184437 | mse 0.018365 | lpips 0.000000\n",
      "Step 15150 | lossG 0.114942 | rate bpp 4.135566 | mse 0.008186 | lpips 0.000000\n",
      "Step 15200 | lossG 0.113030 | rate bpp 4.171713 | mse 0.005980 | lpips 0.000000\n",
      "Step 15250 | lossG 0.112378 | rate bpp 4.141598 | mse 0.005011 | lpips 0.000000\n",
      "Step 15300 | lossG 0.114730 | rate bpp 4.121478 | mse 0.004035 | lpips 0.000000\n",
      "Step 15350 | lossG 0.118586 | rate bpp 4.171713 | mse 0.010042 | lpips 0.000000\n",
      "Step 15400 | lossG 0.118237 | rate bpp 4.108225 | mse 0.005389 | lpips 0.000000\n",
      "Step 15450 | lossG 0.117744 | rate bpp 4.139000 | mse 0.013770 | lpips 0.000000\n",
      "Step 15500 | lossG 0.122916 | rate bpp 4.126937 | mse 0.008509 | lpips 0.000000\n",
      "Step 15550 | lossG 0.130062 | rate bpp 4.123459 | mse 0.012452 | lpips 0.000000\n",
      "Step 15600 | lossG 0.110743 | rate bpp 4.133453 | mse 0.007619 | lpips 0.000000\n",
      "Step 15650 | lossG 0.134528 | rate bpp 4.161631 | mse 0.019173 | lpips 0.000000\n",
      "Step 15700 | lossG 0.115657 | rate bpp 4.188620 | mse 0.007707 | lpips 0.000000\n",
      "Step 15750 | lossG 0.115173 | rate bpp 4.144328 | mse 0.008291 | lpips 0.000000\n",
      "Step 15800 | lossG 0.111981 | rate bpp 4.115622 | mse 0.005572 | lpips 0.000000\n",
      "Step 15850 | lossG 0.111670 | rate bpp 4.125660 | mse 0.005104 | lpips 0.000000\n",
      "Step 15900 | lossG 0.113730 | rate bpp 4.134906 | mse 0.005350 | lpips 0.000000\n",
      "Step 15950 | lossG 0.116494 | rate bpp 4.139132 | mse 0.011849 | lpips 0.000000\n",
      "Step 16000 | lossG 0.114667 | rate bpp 4.151724 | mse 0.009050 | lpips 0.000000\n",
      "Saved checkpoint 16000\n",
      "Step 16050 | lossG 0.116363 | rate bpp 4.151505 | mse 0.007028 | lpips 0.000000\n",
      "Step 16100 | lossG 0.115779 | rate bpp 4.200023 | mse 0.010178 | lpips 0.000000\n",
      "Step 16150 | lossG 0.124986 | rate bpp 4.131472 | mse 0.011940 | lpips 0.000000\n",
      "Step 16200 | lossG 0.121305 | rate bpp 4.163568 | mse 0.009478 | lpips 0.000000\n",
      "Step 16250 | lossG 0.113669 | rate bpp 4.130723 | mse 0.006118 | lpips 0.000000\n",
      "Step 16300 | lossG 0.108746 | rate bpp 4.109854 | mse 0.002844 | lpips 0.000000\n",
      "Step 16350 | lossG 0.115710 | rate bpp 4.120289 | mse 0.007954 | lpips 0.000000\n",
      "Step 16400 | lossG 0.111342 | rate bpp 4.120157 | mse 0.003872 | lpips 0.000000\n",
      "Step 16450 | lossG 0.114948 | rate bpp 4.078463 | mse 0.002752 | lpips 0.000000\n",
      "Step 16500 | lossG 0.116500 | rate bpp 4.139661 | mse 0.009217 | lpips 0.000000\n",
      "Step 16550 | lossG 0.128562 | rate bpp 4.105011 | mse 0.008873 | lpips 0.000000\n",
      "Step 16600 | lossG 0.114404 | rate bpp 4.145429 | mse 0.003434 | lpips 0.000000\n",
      "Step 16650 | lossG 0.113860 | rate bpp 4.120201 | mse 0.006220 | lpips 0.000000\n",
      "Step 16700 | lossG 0.120145 | rate bpp 4.120729 | mse 0.009311 | lpips 0.000000\n",
      "Step 16750 | lossG 0.111752 | rate bpp 4.152649 | mse 0.005256 | lpips 0.000000\n",
      "Step 16800 | lossG 0.109980 | rate bpp 4.107741 | mse 0.005604 | lpips 0.000000\n",
      "Step 16850 | lossG 0.108324 | rate bpp 4.114081 | mse 0.003789 | lpips 0.000000\n",
      "Step 16900 | lossG 0.110919 | rate bpp 4.124648 | mse 0.005256 | lpips 0.000000\n",
      "Step 16950 | lossG 0.112815 | rate bpp 4.107477 | mse 0.006260 | lpips 0.000000\n",
      "Step 17000 | lossG 0.114377 | rate bpp 4.128346 | mse 0.008714 | lpips 0.000000\n",
      "Saved checkpoint 17000\n",
      "Step 17050 | lossG 0.110172 | rate bpp 4.082513 | mse 0.004633 | lpips 0.000000\n",
      "Step 17100 | lossG 0.113083 | rate bpp 4.155115 | mse 0.009464 | lpips 0.000000\n",
      "Step 17150 | lossG 0.106092 | rate bpp 4.093168 | mse 0.003333 | lpips 0.000000\n",
      "Step 17200 | lossG 0.108909 | rate bpp 4.107477 | mse 0.002972 | lpips 0.000000\n",
      "Step 17250 | lossG 0.110214 | rate bpp 4.113773 | mse 0.003905 | lpips 0.000000\n",
      "Step 17300 | lossG 0.115732 | rate bpp 4.136931 | mse 0.008987 | lpips 0.000000\n",
      "Step 17350 | lossG 0.111344 | rate bpp 4.082689 | mse 0.004496 | lpips 0.000000\n",
      "Step 17400 | lossG 0.118173 | rate bpp 4.134290 | mse 0.009163 | lpips 0.000000\n",
      "Step 17450 | lossG 0.113532 | rate bpp 4.128346 | mse 0.003939 | lpips 0.000000\n",
      "Step 17500 | lossG 0.119142 | rate bpp 4.115138 | mse 0.011917 | lpips 0.000000\n",
      "Step 17550 | lossG 0.115220 | rate bpp 4.150448 | mse 0.005260 | lpips 0.000000\n",
      "Step 17600 | lossG 0.110167 | rate bpp 4.187035 | mse 0.017244 | lpips 0.000000\n",
      "Step 17650 | lossG 0.112696 | rate bpp 4.139397 | mse 0.006354 | lpips 0.000000\n",
      "Step 17700 | lossG 0.111375 | rate bpp 4.085815 | mse 0.006017 | lpips 0.000000\n",
      "Step 17750 | lossG 0.113372 | rate bpp 4.134510 | mse 0.005818 | lpips 0.000000\n",
      "Step 17800 | lossG 0.112072 | rate bpp 4.101489 | mse 0.008347 | lpips 0.000000\n",
      "Step 17850 | lossG 0.114061 | rate bpp 4.134554 | mse 0.006152 | lpips 0.000000\n",
      "Step 17900 | lossG 0.113310 | rate bpp 4.142038 | mse 0.008005 | lpips 0.000000\n",
      "Step 17950 | lossG 0.114729 | rate bpp 4.111703 | mse 0.008544 | lpips 0.000000\n",
      "Step 18000 | lossG 0.113622 | rate bpp 4.131296 | mse 0.007996 | lpips 0.000000\n",
      "Saved checkpoint 18000\n",
      "Step 18050 | lossG 0.113431 | rate bpp 4.112011 | mse 0.004892 | lpips 0.000000\n",
      "Step 18100 | lossG 0.111146 | rate bpp 4.102986 | mse 0.008778 | lpips 0.000000\n",
      "Step 18150 | lossG 0.114403 | rate bpp 4.166474 | mse 0.009489 | lpips 0.000000\n",
      "Step 18200 | lossG 0.111046 | rate bpp 4.104967 | mse 0.004196 | lpips 0.000000\n",
      "Step 18250 | lossG 0.111513 | rate bpp 4.104703 | mse 0.005225 | lpips 0.000000\n",
      "Step 18300 | lossG 0.111803 | rate bpp 4.113245 | mse 0.005324 | lpips 0.000000\n",
      "Step 18350 | lossG 0.111783 | rate bpp 4.105628 | mse 0.005830 | lpips 0.000000\n",
      "Step 18400 | lossG 0.111005 | rate bpp 4.113068 | mse 0.004674 | lpips 0.000000\n",
      "Step 18450 | lossG 0.119872 | rate bpp 4.150403 | mse 0.014229 | lpips 0.000000\n",
      "Step 18500 | lossG 0.113410 | rate bpp 4.192186 | mse 0.007416 | lpips 0.000000\n",
      "Step 18550 | lossG 0.112324 | rate bpp 4.140850 | mse 0.006123 | lpips 0.000000\n",
      "Step 18600 | lossG 0.113436 | rate bpp 4.119056 | mse 0.007568 | lpips 0.000000\n",
      "Step 18650 | lossG 0.111180 | rate bpp 4.144460 | mse 0.005627 | lpips 0.000000\n",
      "Step 18700 | lossG 0.113773 | rate bpp 4.134906 | mse 0.008819 | lpips 0.000000\n",
      "Step 18750 | lossG 0.115835 | rate bpp 4.132264 | mse 0.012282 | lpips 0.000000\n",
      "Step 18800 | lossG 0.111976 | rate bpp 4.148995 | mse 0.005316 | lpips 0.000000\n",
      "Step 18850 | lossG 0.113061 | rate bpp 4.079959 | mse 0.009079 | lpips 0.000000\n",
      "Step 18900 | lossG 0.116920 | rate bpp 4.198834 | mse 0.011748 | lpips 0.000000\n",
      "Step 18950 | lossG 0.112286 | rate bpp 4.112672 | mse 0.010370 | lpips 0.000000\n",
      "Step 19000 | lossG 0.116032 | rate bpp 4.105143 | mse 0.009910 | lpips 0.000000\n",
      "Saved checkpoint 19000\n",
      "Step 19050 | lossG 0.111331 | rate bpp 4.135566 | mse 0.006881 | lpips 0.000000\n",
      "Step 19100 | lossG 0.119596 | rate bpp 4.120817 | mse 0.008653 | lpips 0.000000\n",
      "Step 19150 | lossG 0.108597 | rate bpp 4.139176 | mse 0.006107 | lpips 0.000000\n",
      "Step 19200 | lossG 0.106305 | rate bpp 4.154102 | mse 0.007243 | lpips 0.000000\n",
      "Step 19250 | lossG 0.113326 | rate bpp 4.118836 | mse 0.010001 | lpips 0.000000\n",
      "Step 19300 | lossG 0.124477 | rate bpp 4.157316 | mse 0.012109 | lpips 0.000000\n",
      "Step 19350 | lossG 0.114512 | rate bpp 4.160706 | mse 0.007666 | lpips 0.000000\n",
      "Step 19400 | lossG 0.113001 | rate bpp 4.162731 | mse 0.006591 | lpips 0.000000\n",
      "Step 19450 | lossG 0.113640 | rate bpp 4.140938 | mse 0.007294 | lpips 0.000000\n",
      "Step 19500 | lossG 0.108021 | rate bpp 4.157316 | mse 0.011660 | lpips 0.000000\n",
      "Step 19550 | lossG 0.118020 | rate bpp 4.122138 | mse 0.011700 | lpips 0.000000\n",
      "Step 19600 | lossG 0.119511 | rate bpp 4.130943 | mse 0.010984 | lpips 0.000000\n",
      "Step 19650 | lossG 0.111895 | rate bpp 4.142214 | mse 0.002685 | lpips 0.000000\n",
      "Step 19700 | lossG 0.098716 | rate bpp 4.106332 | mse 0.004351 | lpips 0.000000\n",
      "Step 19750 | lossG 0.103450 | rate bpp 4.118616 | mse 0.006066 | lpips 0.000000\n",
      "Step 19800 | lossG 0.110012 | rate bpp 4.124207 | mse 0.003735 | lpips 0.000000\n",
      "Step 19850 | lossG 0.114390 | rate bpp 4.124163 | mse 0.009769 | lpips 0.000000\n",
      "Step 19900 | lossG 0.112996 | rate bpp 4.147630 | mse 0.007057 | lpips 0.000000\n",
      "Step 19950 | lossG 0.109377 | rate bpp 4.110118 | mse 0.003031 | lpips 0.000000\n",
      "Step 20000 | lossG 0.115820 | rate bpp 4.147233 | mse 0.011389 | lpips 0.000000\n",
      "Saved checkpoint 20000\n",
      "Step 20050 | lossG 0.108550 | rate bpp 4.102854 | mse 0.004514 | lpips 0.000000\n",
      "Step 20100 | lossG 0.118329 | rate bpp 4.126452 | mse 0.012685 | lpips 0.000000\n",
      "Step 20150 | lossG 0.121209 | rate bpp 4.127950 | mse 0.011816 | lpips 0.000000\n",
      "Step 20200 | lossG 0.114872 | rate bpp 4.149567 | mse 0.006876 | lpips 0.000000\n",
      "Step 20250 | lossG 0.136301 | rate bpp 4.142919 | mse 0.027316 | lpips 0.000000\n",
      "Step 20300 | lossG 0.114625 | rate bpp 4.161895 | mse 0.008127 | lpips 0.000000\n",
      "Step 20350 | lossG 0.116161 | rate bpp 4.128830 | mse 0.014424 | lpips 0.000000\n",
      "Step 20400 | lossG 0.116232 | rate bpp 4.202708 | mse 0.010405 | lpips 0.000000\n",
      "Step 20450 | lossG 0.113242 | rate bpp 4.150228 | mse 0.007780 | lpips 0.000000\n",
      "Step 20500 | lossG 0.109295 | rate bpp 4.123415 | mse 0.003622 | lpips 0.000000\n",
      "Step 20550 | lossG 0.112503 | rate bpp 4.078991 | mse 0.003939 | lpips 0.000000\n",
      "Step 20600 | lossG 0.120889 | rate bpp 4.131120 | mse 0.016418 | lpips 0.000000\n",
      "Step 20650 | lossG 0.114093 | rate bpp 4.144680 | mse 0.008511 | lpips 0.000000\n",
      "Step 20700 | lossG 0.116341 | rate bpp 4.163568 | mse 0.010799 | lpips 0.000000\n",
      "Step 20750 | lossG 0.115729 | rate bpp 4.123811 | mse 0.008097 | lpips 0.000000\n",
      "Step 20800 | lossG 0.125056 | rate bpp 4.130195 | mse 0.015083 | lpips 0.000000\n",
      "Step 20850 | lossG 0.116608 | rate bpp 4.126056 | mse 0.009850 | lpips 0.000000\n",
      "Step 20900 | lossG 0.112197 | rate bpp 4.165593 | mse 0.007847 | lpips 0.000000\n",
      "Step 20950 | lossG 0.108948 | rate bpp 4.100697 | mse 0.003466 | lpips 0.000000\n",
      "Step 21000 | lossG 0.115529 | rate bpp 4.160574 | mse 0.008142 | lpips 0.000000\n",
      "Saved checkpoint 21000\n",
      "Step 21050 | lossG 0.111359 | rate bpp 4.181795 | mse 0.010987 | lpips 0.000000\n",
      "Step 21100 | lossG 0.116312 | rate bpp 4.138692 | mse 0.008715 | lpips 0.000000\n",
      "Step 21150 | lossG 0.109820 | rate bpp 4.116282 | mse 0.002229 | lpips 0.000000\n",
      "Step 21200 | lossG 0.116580 | rate bpp 4.074060 | mse 0.010845 | lpips 0.000000\n",
      "Step 21250 | lossG 0.113131 | rate bpp 4.138340 | mse 0.005623 | lpips 0.000000\n",
      "Step 21300 | lossG 0.119627 | rate bpp 4.156919 | mse 0.016298 | lpips 0.000000\n",
      "Step 21350 | lossG 0.110821 | rate bpp 4.110030 | mse 0.006101 | lpips 0.000000\n",
      "Step 21400 | lossG 0.112300 | rate bpp 4.186638 | mse 0.007933 | lpips 0.000000\n",
      "Step 21450 | lossG 0.117949 | rate bpp 4.120861 | mse 0.007770 | lpips 0.000000\n",
      "Step 21500 | lossG 0.121150 | rate bpp 4.124691 | mse 0.015549 | lpips 0.000000\n",
      "Step 21550 | lossG 0.120162 | rate bpp 4.165946 | mse 0.006463 | lpips 0.000000\n",
      "Step 21600 | lossG 0.113181 | rate bpp 4.151769 | mse 0.008594 | lpips 0.000000\n",
      "Step 21650 | lossG 0.120162 | rate bpp 4.150184 | mse 0.012066 | lpips 0.000000\n",
      "Step 21700 | lossG 0.111436 | rate bpp 4.123723 | mse 0.005362 | lpips 0.000000\n",
      "Step 21750 | lossG 0.108169 | rate bpp 4.117867 | mse 0.002229 | lpips 0.000000\n",
      "Step 21800 | lossG 0.119392 | rate bpp 4.094665 | mse 0.009679 | lpips 0.000000\n",
      "Step 21850 | lossG 0.116159 | rate bpp 4.138560 | mse 0.005531 | lpips 0.000000\n",
      "Step 21900 | lossG 0.113781 | rate bpp 4.128654 | mse 0.006908 | lpips 0.000000\n",
      "Step 21950 | lossG 0.113662 | rate bpp 4.139308 | mse 0.007736 | lpips 0.000000\n",
      "Step 22000 | lossG 0.111087 | rate bpp 4.128698 | mse 0.004844 | lpips 0.000000\n",
      "Saved checkpoint 22000\n",
      "Step 22050 | lossG 0.113713 | rate bpp 4.187299 | mse 0.012987 | lpips 0.000000\n",
      "Step 22100 | lossG 0.121980 | rate bpp 4.141818 | mse 0.008495 | lpips 0.000000\n",
      "Step 22150 | lossG 0.107423 | rate bpp 4.108049 | mse 0.004034 | lpips 0.000000\n",
      "Step 22200 | lossG 0.110467 | rate bpp 4.092992 | mse 0.004207 | lpips 0.000000\n",
      "Step 22250 | lossG 0.109436 | rate bpp 4.101181 | mse 0.005143 | lpips 0.000000\n",
      "Step 22300 | lossG 0.113675 | rate bpp 4.119452 | mse 0.006377 | lpips 0.000000\n",
      "Step 22350 | lossG 0.110374 | rate bpp 4.097923 | mse 0.004708 | lpips 0.000000\n",
      "Step 22400 | lossG 0.107031 | rate bpp 4.095721 | mse 0.005438 | lpips 0.000000\n",
      "Step 22450 | lossG 0.113998 | rate bpp 4.123150 | mse 0.005480 | lpips 0.000000\n",
      "Step 22500 | lossG 0.112277 | rate bpp 4.146969 | mse 0.006915 | lpips 0.000000\n",
      "Step 22550 | lossG 0.128380 | rate bpp 4.138032 | mse 0.011933 | lpips 0.000000\n",
      "Step 22600 | lossG 0.105954 | rate bpp 4.160530 | mse 0.009183 | lpips 0.000000\n",
      "Step 22650 | lossG 0.119543 | rate bpp 4.124735 | mse 0.009762 | lpips 0.000000\n",
      "Step 22700 | lossG 0.112859 | rate bpp 4.154014 | mse 0.007606 | lpips 0.000000\n",
      "Step 22750 | lossG 0.110418 | rate bpp 4.128082 | mse 0.007215 | lpips 0.000000\n",
      "Step 22800 | lossG 0.112347 | rate bpp 4.127950 | mse 0.006242 | lpips 0.000000\n",
      "Step 22850 | lossG 0.115283 | rate bpp 4.120201 | mse 0.005686 | lpips 0.000000\n",
      "Step 22900 | lossG 0.111283 | rate bpp 4.100168 | mse 0.005326 | lpips 0.000000\n",
      "Step 22950 | lossG 0.121979 | rate bpp 4.159561 | mse 0.010002 | lpips 0.000000\n",
      "Step 23000 | lossG 0.119384 | rate bpp 4.148643 | mse 0.011514 | lpips 0.000000\n",
      "Saved checkpoint 23000\n",
      "Step 23050 | lossG 0.112964 | rate bpp 4.156744 | mse 0.005894 | lpips 0.000000\n",
      "Step 23100 | lossG 0.113590 | rate bpp 4.098231 | mse 0.004162 | lpips 0.000000\n",
      "Step 23150 | lossG 0.114380 | rate bpp 4.110207 | mse 0.005251 | lpips 0.000000\n",
      "Step 23200 | lossG 0.112072 | rate bpp 4.148202 | mse 0.008600 | lpips 0.000000\n",
      "Step 23250 | lossG 0.116432 | rate bpp 4.133145 | mse 0.009025 | lpips 0.000000\n",
      "Step 23300 | lossG 0.115655 | rate bpp 4.117383 | mse 0.005622 | lpips 0.000000\n",
      "Step 23350 | lossG 0.117147 | rate bpp 4.128478 | mse 0.006523 | lpips 0.000000\n",
      "Step 23400 | lossG 0.108224 | rate bpp 4.103030 | mse 0.002707 | lpips 0.000000\n",
      "Step 23450 | lossG 0.112813 | rate bpp 4.090218 | mse 0.006336 | lpips 0.000000\n",
      "Step 23500 | lossG 0.115924 | rate bpp 4.100036 | mse 0.004492 | lpips 0.000000\n",
      "Step 23550 | lossG 0.115394 | rate bpp 4.131384 | mse 0.006917 | lpips 0.000000\n",
      "Step 23600 | lossG 0.111789 | rate bpp 4.123018 | mse 0.004568 | lpips 0.000000\n",
      "Step 23650 | lossG 0.104904 | rate bpp 4.165637 | mse 0.007023 | lpips 0.000000\n",
      "Step 23700 | lossG 0.130838 | rate bpp 4.148290 | mse 0.023297 | lpips 0.000000\n",
      "Step 23750 | lossG 0.120500 | rate bpp 4.203897 | mse 0.013090 | lpips 0.000000\n",
      "Step 23800 | lossG 0.114178 | rate bpp 4.167971 | mse 0.005913 | lpips 0.000000\n",
      "Step 23850 | lossG 0.116910 | rate bpp 4.138032 | mse 0.007754 | lpips 0.000000\n",
      "Step 23900 | lossG 0.112948 | rate bpp 4.132969 | mse 0.005254 | lpips 0.000000\n",
      "Step 23950 | lossG 0.117926 | rate bpp 4.123239 | mse 0.010683 | lpips 0.000000\n",
      "Step 24000 | lossG 0.112237 | rate bpp 4.100389 | mse 0.005953 | lpips 0.000000\n",
      "Saved checkpoint 24000\n",
      "Step 24050 | lossG 0.109933 | rate bpp 4.098891 | mse 0.005778 | lpips 0.000000\n",
      "Step 24100 | lossG 0.115685 | rate bpp 4.156611 | mse 0.006308 | lpips 0.000000\n",
      "Step 24150 | lossG 0.118671 | rate bpp 4.149435 | mse 0.011935 | lpips 0.000000\n",
      "Step 24200 | lossG 0.107043 | rate bpp 4.182544 | mse 0.010998 | lpips 0.000000\n",
      "Step 24250 | lossG 0.113260 | rate bpp 4.149963 | mse 0.005765 | lpips 0.000000\n",
      "Step 24300 | lossG 0.113777 | rate bpp 4.155070 | mse 0.009071 | lpips 0.000000\n",
      "Step 24350 | lossG 0.131262 | rate bpp 4.147014 | mse 0.015608 | lpips 0.000000\n",
      "Step 24400 | lossG 0.113966 | rate bpp 4.083306 | mse 0.004910 | lpips 0.000000\n",
      "Step 24450 | lossG 0.099318 | rate bpp 4.134642 | mse 0.011708 | lpips 0.000000\n",
      "Step 24500 | lossG 0.117554 | rate bpp 4.216269 | mse 0.009482 | lpips 0.000000\n",
      "Step 24550 | lossG 0.105466 | rate bpp 4.097174 | mse 0.005977 | lpips 0.000000\n",
      "Step 24600 | lossG 0.119358 | rate bpp 4.133321 | mse 0.008654 | lpips 0.000000\n",
      "Step 24650 | lossG 0.110782 | rate bpp 4.155115 | mse 0.004951 | lpips 0.000000\n",
      "Step 24700 | lossG 0.112209 | rate bpp 4.138032 | mse 0.005001 | lpips 0.000000\n",
      "Step 24750 | lossG 0.116550 | rate bpp 4.198790 | mse 0.008192 | lpips 0.000000\n",
      "Step 24800 | lossG 0.113777 | rate bpp 4.130856 | mse 0.005427 | lpips 0.000000\n",
      "Step 24850 | lossG 0.111121 | rate bpp 4.110867 | mse 0.002590 | lpips 0.000000\n",
      "Step 24900 | lossG 0.124120 | rate bpp 4.151152 | mse 0.012561 | lpips 0.000000\n",
      "Step 24950 | lossG 0.115150 | rate bpp 4.107388 | mse 0.007177 | lpips 0.000000\n",
      "Step 25000 | lossG 0.106898 | rate bpp 4.100124 | mse 0.003066 | lpips 0.000000\n",
      "Saved checkpoint 25000\n",
      "Step 25050 | lossG 0.117884 | rate bpp 4.139176 | mse 0.012770 | lpips 0.000000\n",
      "Step 25100 | lossG 0.114914 | rate bpp 4.099728 | mse 0.004368 | lpips 0.000000\n",
      "Step 25150 | lossG 0.114984 | rate bpp 4.145297 | mse 0.007687 | lpips 0.000000\n",
      "Step 25200 | lossG 0.108109 | rate bpp 4.086299 | mse 0.004309 | lpips 0.000000\n",
      "Step 25250 | lossG 0.115085 | rate bpp 4.131912 | mse 0.008298 | lpips 0.000000\n",
      "Step 25300 | lossG 0.115082 | rate bpp 4.129931 | mse 0.009446 | lpips 0.000000\n",
      "Step 25350 | lossG 0.116304 | rate bpp 4.088633 | mse 0.006285 | lpips 0.000000\n",
      "Step 25400 | lossG 0.111779 | rate bpp 4.095281 | mse 0.005107 | lpips 0.000000\n",
      "Step 25450 | lossG 0.113095 | rate bpp 4.085111 | mse 0.004868 | lpips 0.000000\n",
      "Step 25500 | lossG 0.116330 | rate bpp 4.086299 | mse 0.005813 | lpips 0.000000\n",
      "Step 25550 | lossG 0.114815 | rate bpp 4.157140 | mse 0.008556 | lpips 0.000000\n",
      "Step 25600 | lossG 0.112343 | rate bpp 4.141906 | mse 0.006730 | lpips 0.000000\n",
      "Step 25650 | lossG 0.114418 | rate bpp 4.130371 | mse 0.008575 | lpips 0.000000\n",
      "Step 25700 | lossG 0.116772 | rate bpp 4.100829 | mse 0.009079 | lpips 0.000000\n",
      "Step 25750 | lossG 0.138811 | rate bpp 4.158196 | mse 0.013258 | lpips 0.000000\n",
      "Step 25800 | lossG 0.113255 | rate bpp 4.143755 | mse 0.005065 | lpips 0.000000\n",
      "Step 25850 | lossG 0.116207 | rate bpp 4.132749 | mse 0.006276 | lpips 0.000000\n",
      "Step 25900 | lossG 0.111962 | rate bpp 4.128037 | mse 0.005772 | lpips 0.000000\n",
      "Step 25950 | lossG 0.112022 | rate bpp 4.117163 | mse 0.004734 | lpips 0.000000\n",
      "Step 26000 | lossG 0.111413 | rate bpp 4.102325 | mse 0.003882 | lpips 0.000000\n",
      "Saved checkpoint 26000\n",
      "Step 26050 | lossG 0.112199 | rate bpp 4.125088 | mse 0.007274 | lpips 0.000000\n",
      "Step 26100 | lossG 0.112294 | rate bpp 4.136315 | mse 0.005702 | lpips 0.000000\n",
      "Step 26150 | lossG 0.113795 | rate bpp 4.171449 | mse 0.008765 | lpips 0.000000\n",
      "Step 26200 | lossG 0.111535 | rate bpp 4.111395 | mse 0.004442 | lpips 0.000000\n",
      "Step 26250 | lossG 0.112199 | rate bpp 4.110691 | mse 0.007148 | lpips 0.000000\n",
      "Step 26300 | lossG 0.114071 | rate bpp 4.111351 | mse 0.006411 | lpips 0.000000\n",
      "Step 26350 | lossG 0.112474 | rate bpp 4.132969 | mse 0.005864 | lpips 0.000000\n",
      "Step 26400 | lossG 0.112273 | rate bpp 4.174090 | mse 0.007753 | lpips 0.000000\n",
      "Step 26450 | lossG 0.117659 | rate bpp 4.146133 | mse 0.012644 | lpips 0.000000\n",
      "Step 26500 | lossG 0.111947 | rate bpp 4.145032 | mse 0.005422 | lpips 0.000000\n",
      "Step 26550 | lossG 0.112259 | rate bpp 4.160002 | mse 0.011473 | lpips 0.000000\n",
      "Step 26600 | lossG 0.112915 | rate bpp 4.137459 | mse 0.003864 | lpips 0.000000\n",
      "Step 26650 | lossG 0.109618 | rate bpp 4.103778 | mse 0.005961 | lpips 0.000000\n",
      "Step 26700 | lossG 0.110188 | rate bpp 4.148907 | mse 0.005630 | lpips 0.000000\n",
      "Step 26750 | lossG 0.112755 | rate bpp 4.148643 | mse 0.006835 | lpips 0.000000\n",
      "Step 26800 | lossG 0.109368 | rate bpp 4.144372 | mse 0.004597 | lpips 0.000000\n",
      "Step 26850 | lossG 0.113453 | rate bpp 4.149171 | mse 0.005196 | lpips 0.000000\n",
      "Step 26900 | lossG 0.106842 | rate bpp 4.141862 | mse 0.010014 | lpips 0.000000\n",
      "Step 26950 | lossG 0.114508 | rate bpp 4.156568 | mse 0.007059 | lpips 0.000000\n",
      "Step 27000 | lossG 0.108154 | rate bpp 4.096734 | mse 0.003415 | lpips 0.000000\n",
      "Saved checkpoint 27000\n",
      "Step 27050 | lossG 0.110507 | rate bpp 4.125704 | mse 0.006999 | lpips 0.000000\n",
      "Step 27100 | lossG 0.114160 | rate bpp 4.143888 | mse 0.008210 | lpips 0.000000\n",
      "Step 27150 | lossG 0.107230 | rate bpp 4.117251 | mse 0.007041 | lpips 0.000000\n",
      "Step 27200 | lossG 0.111103 | rate bpp 4.108841 | mse 0.003666 | lpips 0.000000\n",
      "Step 27250 | lossG 0.109495 | rate bpp 4.124780 | mse 0.006684 | lpips 0.000000\n",
      "Step 27300 | lossG 0.112681 | rate bpp 4.141994 | mse 0.005211 | lpips 0.000000\n",
      "Step 27350 | lossG 0.113198 | rate bpp 4.114037 | mse 0.004413 | lpips 0.000000\n",
      "Step 27400 | lossG 0.110191 | rate bpp 4.120685 | mse 0.006355 | lpips 0.000000\n",
      "Step 27450 | lossG 0.112718 | rate bpp 4.169203 | mse 0.006748 | lpips 0.000000\n",
      "Step 27500 | lossG 0.114175 | rate bpp 4.132969 | mse 0.006980 | lpips 0.000000\n",
      "Step 27550 | lossG 0.112125 | rate bpp 4.174795 | mse 0.010498 | lpips 0.000000\n",
      "Step 27600 | lossG 0.111053 | rate bpp 4.138868 | mse 0.006210 | lpips 0.000000\n",
      "Step 27650 | lossG 0.115798 | rate bpp 4.186727 | mse 0.009645 | lpips 0.000000\n",
      "Step 27700 | lossG 0.116159 | rate bpp 4.142038 | mse 0.007421 | lpips 0.000000\n",
      "Step 27750 | lossG 0.113037 | rate bpp 4.087928 | mse 0.006804 | lpips 0.000000\n",
      "Step 27800 | lossG 0.114764 | rate bpp 4.135346 | mse 0.007134 | lpips 0.000000\n",
      "Step 27850 | lossG 0.114728 | rate bpp 4.151372 | mse 0.008925 | lpips 0.000000\n",
      "Step 27900 | lossG 0.114986 | rate bpp 4.150536 | mse 0.009428 | lpips 0.000000\n",
      "Step 27950 | lossG 0.113092 | rate bpp 4.101885 | mse 0.004494 | lpips 0.000000\n",
      "Step 28000 | lossG 0.112559 | rate bpp 4.117163 | mse 0.006070 | lpips 0.000000\n",
      "Saved checkpoint 28000\n",
      "Step 28050 | lossG 0.112912 | rate bpp 4.144328 | mse 0.010407 | lpips 0.000000\n",
      "Step 28100 | lossG 0.123287 | rate bpp 4.160178 | mse 0.008739 | lpips 0.000000\n",
      "Step 28150 | lossG 0.122991 | rate bpp 4.200111 | mse 0.011992 | lpips 0.000000\n",
      "Step 28200 | lossG 0.108516 | rate bpp 4.163832 | mse 0.006086 | lpips 0.000000\n",
      "Step 28250 | lossG 0.113492 | rate bpp 4.121874 | mse 0.007394 | lpips 0.000000\n",
      "Step 28300 | lossG 0.103888 | rate bpp 4.098847 | mse 0.004802 | lpips 0.000000\n",
      "Step 28350 | lossG 0.105197 | rate bpp 4.117163 | mse 0.002897 | lpips 0.000000\n",
      "Step 28400 | lossG 0.115417 | rate bpp 4.098804 | mse 0.004787 | lpips 0.000000\n",
      "Step 28450 | lossG 0.109928 | rate bpp 4.135830 | mse 0.004572 | lpips 0.000000\n",
      "Step 28500 | lossG 0.113439 | rate bpp 4.144680 | mse 0.008363 | lpips 0.000000\n",
      "Step 28550 | lossG 0.112302 | rate bpp 4.161939 | mse 0.010545 | lpips 0.000000\n",
      "Step 28600 | lossG 0.110362 | rate bpp 4.131516 | mse 0.003820 | lpips 0.000000\n",
      "Step 28650 | lossG 0.108996 | rate bpp 4.117075 | mse 0.004774 | lpips 0.000000\n",
      "Step 28700 | lossG 0.130486 | rate bpp 4.173254 | mse 0.010734 | lpips 0.000000\n",
      "Step 28750 | lossG 0.109986 | rate bpp 4.187211 | mse 0.011452 | lpips 0.000000\n",
      "Step 28800 | lossG 0.112604 | rate bpp 4.114697 | mse 0.007401 | lpips 0.000000\n",
      "Step 28850 | lossG 0.113560 | rate bpp 4.148643 | mse 0.006764 | lpips 0.000000\n",
      "Step 28900 | lossG 0.107896 | rate bpp 4.119893 | mse 0.003894 | lpips 0.000000\n",
      "Step 28950 | lossG 0.113630 | rate bpp 4.144460 | mse 0.009242 | lpips 0.000000\n",
      "Step 29000 | lossG 0.114004 | rate bpp 4.171228 | mse 0.009603 | lpips 0.000000\n",
      "Saved checkpoint 29000\n",
      "Step 29050 | lossG 0.119280 | rate bpp 4.130327 | mse 0.008653 | lpips 0.000000\n",
      "Step 29100 | lossG 0.115931 | rate bpp 4.150932 | mse 0.012717 | lpips 0.000000\n",
      "Step 29150 | lossG 0.110494 | rate bpp 4.172153 | mse 0.006472 | lpips 0.000000\n",
      "Step 29200 | lossG 0.118536 | rate bpp 4.123855 | mse 0.006985 | lpips 0.000000\n",
      "Step 29250 | lossG 0.115120 | rate bpp 4.157888 | mse 0.008974 | lpips 0.000000\n",
      "Step 29300 | lossG 0.114757 | rate bpp 4.096206 | mse 0.007153 | lpips 0.000000\n",
      "Step 29350 | lossG 0.110552 | rate bpp 4.117031 | mse 0.002154 | lpips 0.000000\n",
      "Step 29400 | lossG 0.110341 | rate bpp 4.115490 | mse 0.005662 | lpips 0.000000\n",
      "Step 29450 | lossG 0.115203 | rate bpp 4.129314 | mse 0.009314 | lpips 0.000000\n",
      "Step 29500 | lossG 0.115338 | rate bpp 4.155202 | mse 0.008681 | lpips 0.000000\n",
      "Step 29550 | lossG 0.119881 | rate bpp 4.124075 | mse 0.009593 | lpips 0.000000\n",
      "Step 29600 | lossG 0.113665 | rate bpp 4.124780 | mse 0.008591 | lpips 0.000000\n",
      "Step 29650 | lossG 0.113204 | rate bpp 4.104747 | mse 0.004756 | lpips 0.000000\n",
      "Step 29700 | lossG 0.118733 | rate bpp 4.139044 | mse 0.007902 | lpips 0.000000\n",
      "Step 29750 | lossG 0.111406 | rate bpp 4.109282 | mse 0.007894 | lpips 0.000000\n",
      "Step 29800 | lossG 0.114789 | rate bpp 4.129931 | mse 0.004972 | lpips 0.000000\n",
      "Step 29850 | lossG 0.115786 | rate bpp 4.103250 | mse 0.008703 | lpips 0.000000\n",
      "Step 29900 | lossG 0.125695 | rate bpp 4.095193 | mse 0.007501 | lpips 0.000000\n",
      "Step 29950 | lossG 0.111396 | rate bpp 4.145164 | mse 0.003976 | lpips 0.000000\n",
      "Step 30000 | lossG 0.105935 | rate bpp 4.093784 | mse 0.003295 | lpips 0.000000\n",
      "Saved checkpoint 30000\n",
      "Step 30050 | lossG 0.121444 | rate bpp 4.173474 | mse 0.009998 | lpips 0.000000\n",
      "Step 30100 | lossG 0.108260 | rate bpp 4.124516 | mse 0.004669 | lpips 0.000000\n",
      "Step 30150 | lossG 0.104573 | rate bpp 4.114653 | mse 0.004016 | lpips 0.000000\n",
      "Step 30200 | lossG 0.113467 | rate bpp 4.106200 | mse 0.006390 | lpips 0.000000\n",
      "Step 30250 | lossG 0.112364 | rate bpp 4.139617 | mse 0.007896 | lpips 0.000000\n",
      "Step 30300 | lossG 0.110740 | rate bpp 4.139441 | mse 0.007347 | lpips 0.000000\n",
      "Step 30350 | lossG 0.113664 | rate bpp 4.128214 | mse 0.008526 | lpips 0.000000\n",
      "Step 30400 | lossG 0.112845 | rate bpp 4.145781 | mse 0.005751 | lpips 0.000000\n",
      "Step 30450 | lossG 0.112160 | rate bpp 4.126056 | mse 0.004194 | lpips 0.000000\n",
      "Step 30500 | lossG 0.113582 | rate bpp 4.093388 | mse 0.003542 | lpips 0.000000\n",
      "Step 30550 | lossG 0.100786 | rate bpp 4.097394 | mse 0.002825 | lpips 0.000000\n",
      "Step 30600 | lossG 0.113545 | rate bpp 4.102766 | mse 0.003492 | lpips 0.000000\n",
      "Step 30650 | lossG 0.113389 | rate bpp 4.155731 | mse 0.010477 | lpips 0.000000\n",
      "Step 30700 | lossG 0.122328 | rate bpp 4.123063 | mse 0.008113 | lpips 0.000000\n",
      "Step 30750 | lossG 0.134481 | rate bpp 4.216885 | mse 0.023563 | lpips 0.000000\n",
      "Step 30800 | lossG 0.124667 | rate bpp 4.151637 | mse 0.008805 | lpips 0.000000\n",
      "Step 30850 | lossG 0.117507 | rate bpp 4.184041 | mse 0.010269 | lpips 0.000000\n",
      "Step 30900 | lossG 0.109895 | rate bpp 4.133629 | mse 0.003876 | lpips 0.000000\n",
      "Step 30950 | lossG 0.109736 | rate bpp 4.130151 | mse 0.004674 | lpips 0.000000\n",
      "Step 31000 | lossG 0.117634 | rate bpp 4.174971 | mse 0.016206 | lpips 0.000000\n",
      "Saved checkpoint 31000\n",
      "Step 31050 | lossG 0.119759 | rate bpp 4.143271 | mse 0.005269 | lpips 0.000000\n",
      "Step 31100 | lossG 0.124559 | rate bpp 4.117823 | mse 0.011427 | lpips 0.000000\n",
      "Step 31150 | lossG 0.112268 | rate bpp 4.190777 | mse 0.007819 | lpips 0.000000\n",
      "Step 31200 | lossG 0.110475 | rate bpp 4.123987 | mse 0.004270 | lpips 0.000000\n",
      "Step 31250 | lossG 0.113850 | rate bpp 4.150139 | mse 0.005475 | lpips 0.000000\n",
      "Step 31300 | lossG 0.111284 | rate bpp 4.119408 | mse 0.007281 | lpips 0.000000\n",
      "Step 31350 | lossG 0.109361 | rate bpp 4.097835 | mse 0.003393 | lpips 0.000000\n",
      "Step 31400 | lossG 0.119641 | rate bpp 4.114433 | mse 0.008279 | lpips 0.000000\n",
      "Step 31450 | lossG 0.118305 | rate bpp 4.155555 | mse 0.007570 | lpips 0.000000\n",
      "Step 31500 | lossG 0.108115 | rate bpp 4.129271 | mse 0.005834 | lpips 0.000000\n",
      "Step 31550 | lossG 0.114450 | rate bpp 4.158064 | mse 0.008020 | lpips 0.000000\n",
      "Step 31600 | lossG 0.112338 | rate bpp 4.118659 | mse 0.007789 | lpips 0.000000\n",
      "Step 31650 | lossG 0.114831 | rate bpp 4.155379 | mse 0.009064 | lpips 0.000000\n",
      "Step 31700 | lossG 0.110852 | rate bpp 4.154410 | mse 0.005197 | lpips 0.000000\n",
      "Step 31750 | lossG 0.111056 | rate bpp 4.127025 | mse 0.003763 | lpips 0.000000\n",
      "Step 31800 | lossG 0.119642 | rate bpp 4.140542 | mse 0.009094 | lpips 0.000000\n",
      "Step 31850 | lossG 0.120839 | rate bpp 4.148995 | mse 0.006488 | lpips 0.000000\n",
      "Step 31900 | lossG 0.112527 | rate bpp 4.121346 | mse 0.004714 | lpips 0.000000\n",
      "Step 31950 | lossG 0.113086 | rate bpp 4.135962 | mse 0.007883 | lpips 0.000000\n",
      "Step 32000 | lossG 0.102105 | rate bpp 4.211073 | mse 0.014384 | lpips 0.000000\n",
      "Saved checkpoint 32000\n",
      "Step 32050 | lossG 0.108726 | rate bpp 4.114125 | mse 0.004695 | lpips 0.000000\n",
      "Step 32100 | lossG 0.113426 | rate bpp 4.111571 | mse 0.009391 | lpips 0.000000\n",
      "Step 32150 | lossG 0.115719 | rate bpp 4.161455 | mse 0.007240 | lpips 0.000000\n",
      "Step 32200 | lossG 0.109745 | rate bpp 4.104351 | mse 0.003331 | lpips 0.000000\n",
      "Step 32250 | lossG 0.112250 | rate bpp 4.140057 | mse 0.005569 | lpips 0.000000\n",
      "Step 32300 | lossG 0.120421 | rate bpp 4.114697 | mse 0.005801 | lpips 0.000000\n",
      "Step 32350 | lossG 0.110280 | rate bpp 4.106420 | mse 0.004539 | lpips 0.000000\n",
      "Step 32400 | lossG 0.112391 | rate bpp 4.131252 | mse 0.005019 | lpips 0.000000\n",
      "Step 32450 | lossG 0.108581 | rate bpp 4.090438 | mse 0.002784 | lpips 0.000000\n",
      "Step 32500 | lossG 0.108478 | rate bpp 4.142875 | mse 0.007846 | lpips 0.000000\n",
      "Step 32550 | lossG 0.112604 | rate bpp 4.112100 | mse 0.004541 | lpips 0.000000\n",
      "Step 32600 | lossG 0.117363 | rate bpp 4.137768 | mse 0.013924 | lpips 0.000000\n",
      "Step 32650 | lossG 0.110306 | rate bpp 4.107917 | mse 0.009111 | lpips 0.000000\n",
      "Step 32700 | lossG 0.115826 | rate bpp 4.139441 | mse 0.005051 | lpips 0.000000\n",
      "Step 32750 | lossG 0.110830 | rate bpp 4.153001 | mse 0.004741 | lpips 0.000000\n",
      "Step 32800 | lossG 0.112094 | rate bpp 4.121962 | mse 0.006353 | lpips 0.000000\n",
      "Step 32850 | lossG 0.140876 | rate bpp 4.184525 | mse 0.014279 | lpips 0.000000\n",
      "Step 32900 | lossG 0.113064 | rate bpp 4.141642 | mse 0.008092 | lpips 0.000000\n",
      "Step 32950 | lossG 0.113161 | rate bpp 4.134510 | mse 0.005219 | lpips 0.000000\n",
      "Step 33000 | lossG 0.111229 | rate bpp 4.107741 | mse 0.006115 | lpips 0.000000\n",
      "Saved checkpoint 33000\n",
      "Step 33050 | lossG 0.107448 | rate bpp 4.156523 | mse 0.008193 | lpips 0.000000\n",
      "Step 33100 | lossG 0.118557 | rate bpp 4.152341 | mse 0.007754 | lpips 0.000000\n",
      "Step 33150 | lossG 0.118191 | rate bpp 4.155026 | mse 0.015847 | lpips 0.000000\n",
      "Step 33200 | lossG 0.114004 | rate bpp 4.139925 | mse 0.004785 | lpips 0.000000\n",
      "Step 33250 | lossG 0.115211 | rate bpp 4.146969 | mse 0.006109 | lpips 0.000000\n",
      "Step 33300 | lossG 0.109317 | rate bpp 4.103822 | mse 0.002643 | lpips 0.000000\n",
      "Step 33350 | lossG 0.110700 | rate bpp 4.145561 | mse 0.008312 | lpips 0.000000\n",
      "Step 33400 | lossG 0.117480 | rate bpp 4.105319 | mse 0.010798 | lpips 0.000000\n",
      "Step 33450 | lossG 0.111193 | rate bpp 4.140938 | mse 0.004685 | lpips 0.000000\n",
      "Step 33500 | lossG 0.110219 | rate bpp 4.120597 | mse 0.005018 | lpips 0.000000\n",
      "Step 33550 | lossG 0.114423 | rate bpp 4.101357 | mse 0.005601 | lpips 0.000000\n",
      "Step 33600 | lossG 0.115337 | rate bpp 4.144768 | mse 0.009021 | lpips 0.000000\n",
      "Step 33650 | lossG 0.111262 | rate bpp 4.111527 | mse 0.004229 | lpips 0.000000\n",
      "Step 33700 | lossG 0.109557 | rate bpp 4.140365 | mse 0.006078 | lpips 0.000000\n",
      "Step 33750 | lossG 0.111213 | rate bpp 4.110471 | mse 0.006960 | lpips 0.000000\n",
      "Step 33800 | lossG 0.118603 | rate bpp 4.123150 | mse 0.008463 | lpips 0.000000\n",
      "Step 33850 | lossG 0.107641 | rate bpp 4.142259 | mse 0.006595 | lpips 0.000000\n",
      "Step 33900 | lossG 0.104701 | rate bpp 4.164580 | mse 0.007141 | lpips 0.000000\n",
      "Step 33950 | lossG 0.109820 | rate bpp 4.142259 | mse 0.004951 | lpips 0.000000\n",
      "Step 34000 | lossG 0.111959 | rate bpp 4.130151 | mse 0.004948 | lpips 0.000000\n",
      "Saved checkpoint 34000\n",
      "Step 34050 | lossG 0.110814 | rate bpp 4.145297 | mse 0.007552 | lpips 0.000000\n",
      "Step 34100 | lossG 0.124404 | rate bpp 4.163304 | mse 0.010457 | lpips 0.000000\n",
      "Step 34150 | lossG 0.111864 | rate bpp 4.153221 | mse 0.003959 | lpips 0.000000\n",
      "Step 34200 | lossG 0.114198 | rate bpp 4.130195 | mse 0.008565 | lpips 0.000000\n",
      "Step 34250 | lossG 0.116087 | rate bpp 4.121874 | mse 0.002469 | lpips 0.000000\n",
      "Step 34300 | lossG 0.116631 | rate bpp 4.185274 | mse 0.008494 | lpips 0.000000\n",
      "Step 34350 | lossG 0.115431 | rate bpp 4.138472 | mse 0.007925 | lpips 0.000000\n",
      "Step 34400 | lossG 0.110787 | rate bpp 4.107080 | mse 0.004858 | lpips 0.000000\n",
      "Step 34450 | lossG 0.108369 | rate bpp 4.102237 | mse 0.004182 | lpips 0.000000\n",
      "Step 34500 | lossG 0.112467 | rate bpp 4.093696 | mse 0.004826 | lpips 0.000000\n",
      "Step 34550 | lossG 0.112076 | rate bpp 4.122622 | mse 0.007462 | lpips 0.000000\n",
      "Step 34600 | lossG 0.115559 | rate bpp 4.129535 | mse 0.006974 | lpips 0.000000\n",
      "Step 34650 | lossG 0.113814 | rate bpp 4.109590 | mse 0.005411 | lpips 0.000000\n",
      "Step 34700 | lossG 0.112584 | rate bpp 4.129799 | mse 0.010400 | lpips 0.000000\n",
      "Step 34750 | lossG 0.113699 | rate bpp 4.112980 | mse 0.004697 | lpips 0.000000\n",
      "Step 34800 | lossG 0.111827 | rate bpp 4.139441 | mse 0.005836 | lpips 0.000000\n",
      "Step 34850 | lossG 0.111539 | rate bpp 4.115798 | mse 0.007077 | lpips 0.000000\n",
      "Step 34900 | lossG 0.104284 | rate bpp 4.141202 | mse 0.008073 | lpips 0.000000\n",
      "Step 34950 | lossG 0.115165 | rate bpp 4.134994 | mse 0.009254 | lpips 0.000000\n",
      "Step 35000 | lossG 0.116592 | rate bpp 4.107124 | mse 0.003640 | lpips 0.000000\n",
      "Saved checkpoint 35000\n",
      "Step 35050 | lossG 0.104941 | rate bpp 4.116282 | mse 0.002373 | lpips 0.000000\n",
      "Step 35100 | lossG 0.110651 | rate bpp 4.140542 | mse 0.005316 | lpips 0.000000\n",
      "Step 35150 | lossG 0.111303 | rate bpp 4.094049 | mse 0.005659 | lpips 0.000000\n",
      "Step 35200 | lossG 0.116625 | rate bpp 4.133277 | mse 0.008458 | lpips 0.000000\n",
      "Step 35250 | lossG 0.107618 | rate bpp 4.124867 | mse 0.002405 | lpips 0.000000\n",
      "Step 35300 | lossG 0.117110 | rate bpp 4.157272 | mse 0.009947 | lpips 0.000000\n",
      "Step 35350 | lossG 0.115521 | rate bpp 4.100432 | mse 0.007837 | lpips 0.000000\n",
      "Step 35400 | lossG 0.113657 | rate bpp 4.155070 | mse 0.008365 | lpips 0.000000\n",
      "Step 35450 | lossG 0.110820 | rate bpp 4.125440 | mse 0.002865 | lpips 0.000000\n",
      "Step 35500 | lossG 0.114217 | rate bpp 4.112628 | mse 0.004688 | lpips 0.000000\n",
      "Step 35550 | lossG 0.113833 | rate bpp 4.149655 | mse 0.009060 | lpips 0.000000\n",
      "Step 35600 | lossG 0.106176 | rate bpp 4.133805 | mse 0.005429 | lpips 0.000000\n",
      "Step 35650 | lossG 0.119572 | rate bpp 4.117603 | mse 0.005871 | lpips 0.000000\n",
      "Step 35700 | lossG 0.107885 | rate bpp 4.142478 | mse 0.010416 | lpips 0.000000\n",
      "Step 35750 | lossG 0.112766 | rate bpp 4.161014 | mse 0.006400 | lpips 0.000000\n",
      "Step 35800 | lossG 0.108098 | rate bpp 4.155775 | mse 0.007183 | lpips 0.000000\n",
      "Step 35850 | lossG 0.116682 | rate bpp 4.168851 | mse 0.008613 | lpips 0.000000\n",
      "Step 35900 | lossG 0.109464 | rate bpp 4.130283 | mse 0.005014 | lpips 0.000000\n",
      "Step 35950 | lossG 0.109662 | rate bpp 4.126673 | mse 0.005045 | lpips 0.000000\n",
      "Step 36000 | lossG 0.121675 | rate bpp 4.159738 | mse 0.010840 | lpips 0.000000\n",
      "Saved checkpoint 36000\n",
      "Step 36050 | lossG 0.113142 | rate bpp 4.112364 | mse 0.005343 | lpips 0.000000\n",
      "Step 36100 | lossG 0.114260 | rate bpp 4.124471 | mse 0.006896 | lpips 0.000000\n",
      "Step 36150 | lossG 0.115899 | rate bpp 4.158637 | mse 0.009371 | lpips 0.000000\n",
      "Step 36200 | lossG 0.113575 | rate bpp 4.151769 | mse 0.007100 | lpips 0.000000\n",
      "Step 36250 | lossG 0.120041 | rate bpp 4.139617 | mse 0.012299 | lpips 0.000000\n",
      "Step 36300 | lossG 0.107996 | rate bpp 4.150052 | mse 0.006583 | lpips 0.000000\n",
      "Step 36350 | lossG 0.105589 | rate bpp 4.114741 | mse 0.002599 | lpips 0.000000\n",
      "Step 36400 | lossG 0.112600 | rate bpp 4.106552 | mse 0.003796 | lpips 0.000000\n",
      "Step 36450 | lossG 0.111071 | rate bpp 4.139397 | mse 0.006586 | lpips 0.000000\n",
      "Step 36500 | lossG 0.110721 | rate bpp 4.123943 | mse 0.005530 | lpips 0.000000\n",
      "Step 36550 | lossG 0.123888 | rate bpp 4.191745 | mse 0.011773 | lpips 0.000000\n",
      "Step 36600 | lossG 0.116024 | rate bpp 4.134994 | mse 0.006423 | lpips 0.000000\n",
      "Step 36650 | lossG 0.117807 | rate bpp 4.169071 | mse 0.006522 | lpips 0.000000\n",
      "Step 36700 | lossG 0.108402 | rate bpp 4.137547 | mse 0.005199 | lpips 0.000000\n",
      "Step 36750 | lossG 0.108418 | rate bpp 4.088325 | mse 0.003845 | lpips 0.000000\n",
      "Step 36800 | lossG 0.109694 | rate bpp 4.154586 | mse 0.006681 | lpips 0.000000\n",
      "Step 36850 | lossG 0.113852 | rate bpp 4.130459 | mse 0.006933 | lpips 0.000000\n",
      "Step 36900 | lossG 0.116285 | rate bpp 4.121742 | mse 0.006427 | lpips 0.000000\n",
      "Step 36950 | lossG 0.113487 | rate bpp 4.139793 | mse 0.007759 | lpips 0.000000\n",
      "Step 37000 | lossG 0.112518 | rate bpp 4.118836 | mse 0.005689 | lpips 0.000000\n",
      "Saved checkpoint 37000\n",
      "Step 37050 | lossG 0.117173 | rate bpp 4.142127 | mse 0.009649 | lpips 0.000000\n",
      "Step 37100 | lossG 0.112126 | rate bpp 4.116326 | mse 0.005191 | lpips 0.000000\n",
      "Step 37150 | lossG 0.118130 | rate bpp 4.116634 | mse 0.007213 | lpips 0.000000\n",
      "Step 37200 | lossG 0.116155 | rate bpp 4.140938 | mse 0.006194 | lpips 0.000000\n",
      "Step 37250 | lossG 0.114495 | rate bpp 4.140189 | mse 0.007897 | lpips 0.000000\n",
      "Step 37300 | lossG 0.126268 | rate bpp 4.134026 | mse 0.007202 | lpips 0.000000\n",
      "Step 37350 | lossG 0.115901 | rate bpp 4.141906 | mse 0.007999 | lpips 0.000000\n",
      "Step 37400 | lossG 0.117131 | rate bpp 4.128522 | mse 0.008833 | lpips 0.000000\n",
      "Step 37450 | lossG 0.116716 | rate bpp 4.113861 | mse 0.006650 | lpips 0.000000\n",
      "Step 37500 | lossG 0.109390 | rate bpp 4.132749 | mse 0.005191 | lpips 0.000000\n",
      "Step 37550 | lossG 0.109222 | rate bpp 4.132925 | mse 0.006012 | lpips 0.000000\n",
      "Step 37600 | lossG 0.114376 | rate bpp 4.107609 | mse 0.004667 | lpips 0.000000\n",
      "Step 37650 | lossG 0.122400 | rate bpp 4.165065 | mse 0.012946 | lpips 0.000000\n",
      "Step 37700 | lossG 0.115864 | rate bpp 4.159033 | mse 0.007459 | lpips 0.000000\n",
      "Step 37750 | lossG 0.117091 | rate bpp 4.116106 | mse 0.010314 | lpips 0.000000\n",
      "Step 37800 | lossG 0.120695 | rate bpp 4.170612 | mse 0.009293 | lpips 0.000000\n",
      "Step 37850 | lossG 0.110199 | rate bpp 4.136227 | mse 0.005190 | lpips 0.000000\n",
      "Step 37900 | lossG 0.111286 | rate bpp 4.118351 | mse 0.004427 | lpips 0.000000\n",
      "Step 37950 | lossG 0.109545 | rate bpp 4.092243 | mse 0.003701 | lpips 0.000000\n",
      "Step 38000 | lossG 0.123643 | rate bpp 4.151328 | mse 0.012432 | lpips 0.000000\n",
      "Saved checkpoint 38000\n",
      "Step 38050 | lossG 0.109790 | rate bpp 4.147322 | mse 0.005998 | lpips 0.000000\n",
      "Step 38100 | lossG 0.109313 | rate bpp 4.103602 | mse 0.003325 | lpips 0.000000\n",
      "Step 38150 | lossG 0.120238 | rate bpp 4.143931 | mse 0.007967 | lpips 0.000000\n",
      "Step 38200 | lossG 0.113968 | rate bpp 4.158153 | mse 0.009286 | lpips 0.000000\n",
      "Step 38250 | lossG 0.110281 | rate bpp 4.101401 | mse 0.002812 | lpips 0.000000\n",
      "Step 38300 | lossG 0.110697 | rate bpp 4.106376 | mse 0.004229 | lpips 0.000000\n",
      "Step 38350 | lossG 0.110252 | rate bpp 4.138120 | mse 0.004940 | lpips 0.000000\n",
      "Step 38400 | lossG 0.104036 | rate bpp 4.135170 | mse 0.004359 | lpips 0.000000\n",
      "Step 38450 | lossG 0.117076 | rate bpp 4.171185 | mse 0.007173 | lpips 0.000000\n",
      "Step 38500 | lossG 0.105995 | rate bpp 4.130195 | mse 0.005519 | lpips 0.000000\n",
      "Step 38550 | lossG 0.109529 | rate bpp 4.118043 | mse 0.004800 | lpips 0.000000\n",
      "Step 38600 | lossG 0.118774 | rate bpp 4.116018 | mse 0.006793 | lpips 0.000000\n",
      "Step 38650 | lossG 0.113114 | rate bpp 4.153441 | mse 0.006591 | lpips 0.000000\n",
      "Step 38700 | lossG 0.109021 | rate bpp 4.142346 | mse 0.003775 | lpips 0.000000\n",
      "Step 38750 | lossG 0.116681 | rate bpp 4.162467 | mse 0.008238 | lpips 0.000000\n",
      "Step 38800 | lossG 0.108705 | rate bpp 4.115226 | mse 0.002051 | lpips 0.000000\n",
      "Step 38850 | lossG 0.105818 | rate bpp 4.120773 | mse 0.005312 | lpips 0.000000\n",
      "Step 38900 | lossG 0.109103 | rate bpp 4.174002 | mse 0.010311 | lpips 0.000000\n",
      "Step 38950 | lossG 0.111200 | rate bpp 4.119761 | mse 0.003296 | lpips 0.000000\n",
      "Step 39000 | lossG 0.110334 | rate bpp 4.104571 | mse 0.004394 | lpips 0.000000\n",
      "Saved checkpoint 39000\n",
      "Step 39050 | lossG 0.116536 | rate bpp 4.194739 | mse 0.013434 | lpips 0.000000\n",
      "Step 39100 | lossG 0.109485 | rate bpp 4.100697 | mse 0.005819 | lpips 0.000000\n",
      "Step 39150 | lossG 0.111944 | rate bpp 4.120112 | mse 0.003107 | lpips 0.000000\n",
      "Step 39200 | lossG 0.108397 | rate bpp 4.121786 | mse 0.005700 | lpips 0.000000\n",
      "Step 39250 | lossG 0.117064 | rate bpp 4.103954 | mse 0.005865 | lpips 0.000000\n",
      "Step 39300 | lossG 0.113512 | rate bpp 4.132220 | mse 0.006724 | lpips 0.000000\n",
      "Step 39350 | lossG 0.114320 | rate bpp 4.173430 | mse 0.008973 | lpips 0.000000\n",
      "Step 39400 | lossG 0.117457 | rate bpp 4.140365 | mse 0.007996 | lpips 0.000000\n",
      "Step 39450 | lossG 0.110086 | rate bpp 4.092419 | mse 0.002806 | lpips 0.000000\n",
      "Step 39500 | lossG 0.109160 | rate bpp 4.161719 | mse 0.009899 | lpips 0.000000\n",
      "Step 39550 | lossG 0.111911 | rate bpp 4.139529 | mse 0.006259 | lpips 0.000000\n",
      "Step 39600 | lossG 0.114991 | rate bpp 4.163612 | mse 0.011522 | lpips 0.000000\n",
      "Step 39650 | lossG 0.108916 | rate bpp 4.094489 | mse 0.004552 | lpips 0.000000\n",
      "Step 39700 | lossG 0.120745 | rate bpp 4.099464 | mse 0.007376 | lpips 0.000000\n",
      "Step 39750 | lossG 0.115146 | rate bpp 4.157272 | mse 0.007681 | lpips 0.000000\n",
      "Step 39800 | lossG 0.113070 | rate bpp 4.096998 | mse 0.004508 | lpips 0.000000\n",
      "Step 39850 | lossG 0.107781 | rate bpp 4.118616 | mse 0.004445 | lpips 0.000000\n",
      "Step 39900 | lossG 0.108078 | rate bpp 4.085815 | mse 0.002915 | lpips 0.000000\n",
      "Step 39950 | lossG 0.109454 | rate bpp 4.120509 | mse 0.004471 | lpips 0.000000\n",
      "Step 40000 | lossG 0.110985 | rate bpp 4.155863 | mse 0.006249 | lpips 0.000000\n",
      "Saved checkpoint 40000\n"
     ]
    }
   ],
   "source": [
    "# # Cell 10 — Quick demo run (small). Run this cell to test the pipeline.\n",
    "# if __name__ == '__main__':\n",
    "#     print('Starting small demo run...')\n",
    "#     models = train_loop(max_steps=40050)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d69ed22-7280-4aca-8f24-c59603456ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg PSNR: 21.409 dB | Avg LPIPS: 0.271036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([22.16738169910996,\n",
       "  21.67873405061355,\n",
       "  20.10246828911869,\n",
       "  22.32617131945135,\n",
       "  19.234802182771325,\n",
       "  25.121291969351915,\n",
       "  24.402964387472856,\n",
       "  19.32816729103061,\n",
       "  21.018302995173737,\n",
       "  18.706387094401478],\n",
       " [0.32690465450286865,\n",
       "  0.24369613826274872,\n",
       "  0.21585050225257874,\n",
       "  0.2266412228345871,\n",
       "  0.40402546525001526,\n",
       "  0.22896550595760345,\n",
       "  0.281113862991333,\n",
       "  0.23242506384849548,\n",
       "  0.33271151781082153,\n",
       "  0.21802504360675812])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_and_save(models, n_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead936b-6efd-4864-a175-2abb513ff7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
